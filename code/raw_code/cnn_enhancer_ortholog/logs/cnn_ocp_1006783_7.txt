Running cyclical learning rate for Oligo_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.05.
In training mode.
There 158908 positives and 130382 negatives.
There 22170 positives and 18018 negatives.
Epoch 1/23
290/290 - 228s - loss: 0.6870 - accuracy: 0.5364 - macro_f1: 0.5230 - val_loss: 0.6846 - val_accuracy: 0.5389 - val_macro_f1: 0.4677
Epoch 2/23
290/290 - 228s - loss: 0.6780 - accuracy: 0.5650 - macro_f1: 0.5729 - val_loss: 0.6626 - val_accuracy: 0.6000 - val_macro_f1: 0.6850
Epoch 3/23
290/290 - 228s - loss: 0.6699 - accuracy: 0.5882 - macro_f1: 0.6049 - val_loss: 0.6651 - val_accuracy: 0.6024 - val_macro_f1: 0.6215
Epoch 4/23
290/290 - 228s - loss: 0.6676 - accuracy: 0.5919 - macro_f1: 0.6081 - val_loss: 0.6692 - val_accuracy: 0.5948 - val_macro_f1: 0.5925
Epoch 5/23
290/290 - 228s - loss: 0.6618 - accuracy: 0.6034 - macro_f1: 0.6212 - val_loss: 0.6545 - val_accuracy: 0.6185 - val_macro_f1: 0.6834
Epoch 6/23
290/290 - 228s - loss: 0.6587 - accuracy: 0.6093 - macro_f1: 0.6251 - val_loss: 0.6594 - val_accuracy: 0.6126 - val_macro_f1: 0.6098
Epoch 7/23
290/290 - 228s - loss: 0.6501 - accuracy: 0.6218 - macro_f1: 0.6372 - val_loss: 0.6469 - val_accuracy: 0.6301 - val_macro_f1: 0.6373
Epoch 8/23
290/290 - 228s - loss: 0.6439 - accuracy: 0.6299 - macro_f1: 0.6440 - val_loss: 0.6458 - val_accuracy: 0.6289 - val_macro_f1: 0.6944
Epoch 9/23
290/290 - 228s - loss: 0.6352 - accuracy: 0.6410 - macro_f1: 0.6589 - val_loss: 0.6400 - val_accuracy: 0.6404 - val_macro_f1: 0.6439
Epoch 10/23
290/290 - 228s - loss: 0.6318 - accuracy: 0.6461 - macro_f1: 0.6623 - val_loss: 0.6251 - val_accuracy: 0.6544 - val_macro_f1: 0.7131
Epoch 11/23
290/290 - 228s - loss: 0.6268 - accuracy: 0.6516 - macro_f1: 0.6694 - val_loss: 0.6286 - val_accuracy: 0.6558 - val_macro_f1: 0.6709
Epoch 12/23
290/290 - 228s - loss: 0.6229 - accuracy: 0.6567 - macro_f1: 0.6751 - val_loss: 0.6253 - val_accuracy: 0.6563 - val_macro_f1: 0.6752
Epoch 13/23
290/290 - 228s - loss: 0.6188 - accuracy: 0.6608 - macro_f1: 0.6799 - val_loss: 0.6196 - val_accuracy: 0.6640 - val_macro_f1: 0.6965
Epoch 14/23
290/290 - 228s - loss: 0.6161 - accuracy: 0.6631 - macro_f1: 0.6811 - val_loss: 0.6280 - val_accuracy: 0.6509 - val_macro_f1: 0.6580
Epoch 15/23
290/290 - 228s - loss: 0.6154 - accuracy: 0.6631 - macro_f1: 0.6797 - val_loss: 0.6268 - val_accuracy: 0.6547 - val_macro_f1: 0.6624
Epoch 16/23
290/290 - 228s - loss: 0.6119 - accuracy: 0.6680 - macro_f1: 0.6853 - val_loss: 0.6427 - val_accuracy: 0.6370 - val_macro_f1: 0.6088
Epoch 17/23
290/290 - 228s - loss: 0.6064 - accuracy: 0.6742 - macro_f1: 0.6925 - val_loss: 0.6327 - val_accuracy: 0.6492 - val_macro_f1: 0.6383
Epoch 18/23
290/290 - 228s - loss: 0.6037 - accuracy: 0.6753 - macro_f1: 0.6934 - val_loss: 0.6191 - val_accuracy: 0.6648 - val_macro_f1: 0.6873
Epoch 19/23
290/290 - 228s - loss: 0.5985 - accuracy: 0.6803 - macro_f1: 0.6978 - val_loss: 0.6396 - val_accuracy: 0.6499 - val_macro_f1: 0.6435
Epoch 20/23
290/290 - 228s - loss: 0.5974 - accuracy: 0.6811 - macro_f1: 0.6979 - val_loss: 0.6367 - val_accuracy: 0.6509 - val_macro_f1: 0.6486
Epoch 21/23
290/290 - 228s - loss: 0.5866 - accuracy: 0.6912 - macro_f1: 0.7093 - val_loss: 0.6597 - val_accuracy: 0.6304 - val_macro_f1: 0.5975
Epoch 22/23
290/290 - 229s - loss: 0.5802 - accuracy: 0.6958 - macro_f1: 0.7125 - val_loss: 0.6257 - val_accuracy: 0.6580 - val_macro_f1: 0.6947
Epoch 23/23
290/290 - 229s - loss: 0.5700 - accuracy: 0.7039 - macro_f1: 0.7202 - val_loss: 0.6356 - val_accuracy: 0.6567 - val_macro_f1: 0.6878
Running cyclical learning rate for Oligo_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.05.
In evaluation mode.
There 22170 positives and 18018 negatives.
41/41 - 7s
Accuracy: 0.6532504482436823. 
f1_score: 0.656760851899423.
roc_auc: 0.7075760629887828.
prc_auc: 0.7421046328719584.
Running cyclical learning rate for Oligo_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.1.
In training mode.
There 158908 positives and 130382 negatives.
There 22170 positives and 18018 negatives.
Epoch 1/23
290/290 - 229s - loss: 0.6884 - accuracy: 0.5284 - macro_f1: 0.4881 - val_loss: 0.6770 - val_accuracy: 0.5660 - val_macro_f1: 0.6189
Epoch 2/23
290/290 - 228s - loss: 0.6771 - accuracy: 0.5665 - macro_f1: 0.5782 - val_loss: 0.6678 - val_accuracy: 0.5908 - val_macro_f1: 0.6949
Epoch 3/23
290/290 - 229s - loss: 0.6695 - accuracy: 0.5884 - macro_f1: 0.6061 - val_loss: 0.6590 - val_accuracy: 0.6106 - val_macro_f1: 0.6793
Epoch 4/23
290/290 - 230s - loss: 0.6650 - accuracy: 0.5980 - macro_f1: 0.6163 - val_loss: 0.6577 - val_accuracy: 0.6118 - val_macro_f1: 0.6705
Epoch 5/23
290/290 - 230s - loss: 0.6613 - accuracy: 0.6042 - macro_f1: 0.6210 - val_loss: 0.6586 - val_accuracy: 0.6136 - val_macro_f1: 0.6279
Epoch 6/23
290/290 - 230s - loss: 0.6567 - accuracy: 0.6114 - macro_f1: 0.6250 - val_loss: 0.6449 - val_accuracy: 0.6327 - val_macro_f1: 0.6872
Epoch 7/23
290/290 - 231s - loss: 0.6472 - accuracy: 0.6255 - macro_f1: 0.6401 - val_loss: 0.6372 - val_accuracy: 0.6398 - val_macro_f1: 0.6674
Epoch 8/23
290/290 - 231s - loss: 0.6424 - accuracy: 0.6309 - macro_f1: 0.6455 - val_loss: 0.6392 - val_accuracy: 0.6398 - val_macro_f1: 0.6459
Epoch 9/23
290/290 - 231s - loss: 0.6351 - accuracy: 0.6415 - macro_f1: 0.6568 - val_loss: 0.6390 - val_accuracy: 0.6459 - val_macro_f1: 0.6624
Epoch 10/23
290/290 - 231s - loss: 0.6326 - accuracy: 0.6450 - macro_f1: 0.6613 - val_loss: 0.6523 - val_accuracy: 0.6223 - val_macro_f1: 0.5841
Epoch 11/23
290/290 - 231s - loss: 0.6284 - accuracy: 0.6510 - macro_f1: 0.6677 - val_loss: 0.6702 - val_accuracy: 0.5983 - val_macro_f1: 0.5178
Epoch 12/23
290/290 - 231s - loss: 0.6257 - accuracy: 0.6544 - macro_f1: 0.6717 - val_loss: 0.6424 - val_accuracy: 0.6370 - val_macro_f1: 0.6122
Epoch 13/23
290/290 - 231s - loss: 0.6229 - accuracy: 0.6561 - macro_f1: 0.6722 - val_loss: 0.6223 - val_accuracy: 0.6597 - val_macro_f1: 0.6805
Epoch 14/23
290/290 - 231s - loss: 0.6191 - accuracy: 0.6604 - macro_f1: 0.6783 - val_loss: 0.6387 - val_accuracy: 0.6448 - val_macro_f1: 0.6305
Epoch 15/23
290/290 - 231s - loss: 0.6153 - accuracy: 0.6640 - macro_f1: 0.6820 - val_loss: 0.6186 - val_accuracy: 0.6640 - val_macro_f1: 0.6870
Epoch 16/23
290/290 - 231s - loss: 0.6148 - accuracy: 0.6653 - macro_f1: 0.6826 - val_loss: 0.6181 - val_accuracy: 0.6646 - val_macro_f1: 0.6874
Epoch 17/23
290/290 - 231s - loss: 0.6112 - accuracy: 0.6692 - macro_f1: 0.6869 - val_loss: 0.6240 - val_accuracy: 0.6597 - val_macro_f1: 0.6729
Epoch 18/23
290/290 - 231s - loss: 0.6068 - accuracy: 0.6739 - macro_f1: 0.6924 - val_loss: 0.6364 - val_accuracy: 0.6464 - val_macro_f1: 0.6305
Epoch 19/23
290/290 - 231s - loss: 0.6052 - accuracy: 0.6747 - macro_f1: 0.6916 - val_loss: 0.6175 - val_accuracy: 0.6635 - val_macro_f1: 0.6872
Epoch 20/23
290/290 - 231s - loss: 0.6008 - accuracy: 0.6792 - macro_f1: 0.6970 - val_loss: 0.6199 - val_accuracy: 0.6615 - val_macro_f1: 0.6923
Epoch 21/23
290/290 - 232s - loss: 0.5967 - accuracy: 0.6823 - macro_f1: 0.7003 - val_loss: 0.6184 - val_accuracy: 0.6631 - val_macro_f1: 0.6890
Epoch 22/23
290/290 - 231s - loss: 0.5976 - accuracy: 0.6813 - macro_f1: 0.6970 - val_loss: 0.6382 - val_accuracy: 0.6393 - val_macro_f1: 0.6230
Epoch 23/23
290/290 - 231s - loss: 0.5855 - accuracy: 0.6922 - macro_f1: 0.7089 - val_loss: 0.6251 - val_accuracy: 0.6583 - val_macro_f1: 0.6789
Running cyclical learning rate for Oligo_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.1.
In evaluation mode.
There 22170 positives and 18018 negatives.
41/41 - 7s
Accuracy: 0.6586782935903369. 
f1_score: 0.6591654364373445.
roc_auc: 0.7152365789375261.
prc_auc: 0.7499323616058556.
Running cyclical learning rate for Oligo_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.15.
In training mode.
There 158908 positives and 130382 negatives.
There 22170 positives and 18018 negatives.
Epoch 1/23
290/290 - 229s - loss: 0.6863 - accuracy: 0.5374 - macro_f1: 0.5200 - val_loss: 0.6774 - val_accuracy: 0.5700 - val_macro_f1: 0.5936
Epoch 2/23
290/290 - 228s - loss: 0.6754 - accuracy: 0.5747 - macro_f1: 0.5867 - val_loss: 0.6612 - val_accuracy: 0.6021 - val_macro_f1: 0.6833
Epoch 3/23
290/290 - 228s - loss: 0.6683 - accuracy: 0.5918 - macro_f1: 0.6098 - val_loss: 0.6599 - val_accuracy: 0.6023 - val_macro_f1: 0.7056
Epoch 4/23
290/290 - 228s - loss: 0.6661 - accuracy: 0.5950 - macro_f1: 0.6093 - val_loss: 0.6680 - val_accuracy: 0.5916 - val_macro_f1: 0.5927
Epoch 5/23
290/290 - 228s - loss: 0.6624 - accuracy: 0.6020 - macro_f1: 0.6146 - val_loss: 0.6596 - val_accuracy: 0.6113 - val_macro_f1: 0.6186
Epoch 6/23
290/290 - 228s - loss: 0.6552 - accuracy: 0.6121 - macro_f1: 0.6251 - val_loss: 0.6570 - val_accuracy: 0.6207 - val_macro_f1: 0.6166
Epoch 7/23
290/290 - 228s - loss: 0.6472 - accuracy: 0.6240 - macro_f1: 0.6373 - val_loss: 0.6402 - val_accuracy: 0.6379 - val_macro_f1: 0.6907
Epoch 8/23
290/290 - 228s - loss: 0.6429 - accuracy: 0.6298 - macro_f1: 0.6444 - val_loss: 0.6542 - val_accuracy: 0.6169 - val_macro_f1: 0.5788
Epoch 9/23
290/290 - 228s - loss: 0.6396 - accuracy: 0.6357 - macro_f1: 0.6505 - val_loss: 0.6642 - val_accuracy: 0.6045 - val_macro_f1: 0.5404
Epoch 10/23
290/290 - 228s - loss: 0.6340 - accuracy: 0.6427 - macro_f1: 0.6581 - val_loss: 0.6321 - val_accuracy: 0.6420 - val_macro_f1: 0.7186
Epoch 11/23
290/290 - 228s - loss: 0.6322 - accuracy: 0.6446 - macro_f1: 0.6612 - val_loss: 0.6858 - val_accuracy: 0.5799 - val_macro_f1: 0.4680
Epoch 12/23
290/290 - 228s - loss: 0.6273 - accuracy: 0.6516 - macro_f1: 0.6684 - val_loss: 0.6670 - val_accuracy: 0.6084 - val_macro_f1: 0.5472
Epoch 13/23
290/290 - 228s - loss: 0.6255 - accuracy: 0.6524 - macro_f1: 0.6694 - val_loss: 0.6441 - val_accuracy: 0.6372 - val_macro_f1: 0.6113
Epoch 14/23
290/290 - 228s - loss: 0.6219 - accuracy: 0.6576 - macro_f1: 0.6758 - val_loss: 0.6178 - val_accuracy: 0.6653 - val_macro_f1: 0.7021
Epoch 15/23
290/290 - 228s - loss: 0.6205 - accuracy: 0.6593 - macro_f1: 0.6771 - val_loss: 0.6188 - val_accuracy: 0.6621 - val_macro_f1: 0.7113
Epoch 16/23
290/290 - 228s - loss: 0.6180 - accuracy: 0.6612 - macro_f1: 0.6790 - val_loss: 0.6602 - val_accuracy: 0.6075 - val_macro_f1: 0.5360
Epoch 17/23
290/290 - 228s - loss: 0.6167 - accuracy: 0.6625 - macro_f1: 0.6798 - val_loss: 0.6218 - val_accuracy: 0.6614 - val_macro_f1: 0.6783
Epoch 18/23
290/290 - 228s - loss: 0.6149 - accuracy: 0.6649 - macro_f1: 0.6826 - val_loss: 0.6168 - val_accuracy: 0.6614 - val_macro_f1: 0.7161
Epoch 19/23
290/290 - 228s - loss: 0.6150 - accuracy: 0.6654 - macro_f1: 0.6822 - val_loss: 0.6309 - val_accuracy: 0.6529 - val_macro_f1: 0.6498
Epoch 20/23
290/290 - 228s - loss: 0.6138 - accuracy: 0.6661 - macro_f1: 0.6836 - val_loss: 0.6195 - val_accuracy: 0.6651 - val_macro_f1: 0.6798
Epoch 21/23
290/290 - 228s - loss: 0.6110 - accuracy: 0.6685 - macro_f1: 0.6860 - val_loss: 0.6150 - val_accuracy: 0.6624 - val_macro_f1: 0.7210
Epoch 22/23
290/290 - 228s - loss: 0.6126 - accuracy: 0.6671 - macro_f1: 0.6825 - val_loss: 0.6199 - val_accuracy: 0.6625 - val_macro_f1: 0.6898
Epoch 23/23
290/290 - 228s - loss: 0.6021 - accuracy: 0.6776 - macro_f1: 0.6955 - val_loss: 0.6141 - val_accuracy: 0.6701 - val_macro_f1: 0.7051
Running cyclical learning rate for Oligo_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.15.
In evaluation mode.
There 22170 positives and 18018 negatives.
41/41 - 7s
Accuracy: 0.6645316493760337. 
f1_score: 0.6692836224177576.
roc_auc: 0.7219869440437776.
prc_auc: 0.7556812827588505.
Running cyclical learning rate for Oligo_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.2.
In training mode.
There 158908 positives and 130382 negatives.
There 22170 positives and 18018 negatives.
Epoch 1/23
290/290 - 228s - loss: 0.6871 - accuracy: 0.5285 - macro_f1: 0.4758 - val_loss: 0.6767 - val_accuracy: 0.5694 - val_macro_f1: 0.6469
Epoch 2/23
290/290 - 227s - loss: 0.6770 - accuracy: 0.5684 - macro_f1: 0.5853 - val_loss: 0.6840 - val_accuracy: 0.5503 - val_macro_f1: 0.4637
Epoch 3/23
290/290 - 228s - loss: 0.6699 - accuracy: 0.5882 - macro_f1: 0.6066 - val_loss: 0.6623 - val_accuracy: 0.6077 - val_macro_f1: 0.6610
Epoch 4/23
290/290 - 228s - loss: 0.6654 - accuracy: 0.5972 - macro_f1: 0.6157 - val_loss: 0.6633 - val_accuracy: 0.6148 - val_macro_f1: 0.6884
Epoch 5/23
290/290 - 229s - loss: 0.6639 - accuracy: 0.5998 - macro_f1: 0.6179 - val_loss: 0.6576 - val_accuracy: 0.6132 - val_macro_f1: 0.6414
Epoch 6/23
290/290 - 229s - loss: 0.6591 - accuracy: 0.6077 - macro_f1: 0.6222 - val_loss: 0.6458 - val_accuracy: 0.6244 - val_macro_f1: 0.7004
Epoch 7/23
290/290 - 228s - loss: 0.6527 - accuracy: 0.6180 - macro_f1: 0.6326 - val_loss: 0.6756 - val_accuracy: 0.5856 - val_macro_f1: 0.5060
Epoch 8/23
290/290 - 229s - loss: 0.6454 - accuracy: 0.6274 - macro_f1: 0.6402 - val_loss: 0.6449 - val_accuracy: 0.6366 - val_macro_f1: 0.6487
Epoch 9/23
290/290 - 230s - loss: 0.6386 - accuracy: 0.6370 - macro_f1: 0.6536 - val_loss: 0.6277 - val_accuracy: 0.6523 - val_macro_f1: 0.7080
Epoch 10/23
290/290 - 229s - loss: 0.6359 - accuracy: 0.6410 - macro_f1: 0.6572 - val_loss: 0.6369 - val_accuracy: 0.6300 - val_macro_f1: 0.7230
Epoch 11/23
290/290 - 230s - loss: 0.6321 - accuracy: 0.6451 - macro_f1: 0.6626 - val_loss: 0.6293 - val_accuracy: 0.6572 - val_macro_f1: 0.6877
Epoch 12/23
290/290 - 230s - loss: 0.6286 - accuracy: 0.6496 - macro_f1: 0.6675 - val_loss: 0.6226 - val_accuracy: 0.6586 - val_macro_f1: 0.7168
Epoch 13/23
290/290 - 230s - loss: 0.6272 - accuracy: 0.6513 - macro_f1: 0.6691 - val_loss: 0.6527 - val_accuracy: 0.6208 - val_macro_f1: 0.5772
Epoch 14/23
290/290 - 230s - loss: 0.6249 - accuracy: 0.6529 - macro_f1: 0.6702 - val_loss: 0.6323 - val_accuracy: 0.6540 - val_macro_f1: 0.6624
Epoch 15/23
290/290 - 231s - loss: 0.6233 - accuracy: 0.6564 - macro_f1: 0.6744 - val_loss: 0.6221 - val_accuracy: 0.6611 - val_macro_f1: 0.6856
Epoch 16/23
290/290 - 231s - loss: 0.6216 - accuracy: 0.6583 - macro_f1: 0.6765 - val_loss: 0.6191 - val_accuracy: 0.6632 - val_macro_f1: 0.6880
Epoch 17/23
290/290 - 231s - loss: 0.6207 - accuracy: 0.6584 - macro_f1: 0.6758 - val_loss: 0.6179 - val_accuracy: 0.6648 - val_macro_f1: 0.6941
Epoch 18/23
290/290 - 231s - loss: 0.6181 - accuracy: 0.6618 - macro_f1: 0.6800 - val_loss: 0.6290 - val_accuracy: 0.6557 - val_macro_f1: 0.6572
Epoch 19/23
290/290 - 231s - loss: 0.6163 - accuracy: 0.6641 - macro_f1: 0.6812 - val_loss: 0.6297 - val_accuracy: 0.6560 - val_macro_f1: 0.6562
Epoch 20/23
290/290 - 231s - loss: 0.6153 - accuracy: 0.6639 - macro_f1: 0.6819 - val_loss: 0.6154 - val_accuracy: 0.6678 - val_macro_f1: 0.7020
Epoch 21/23
290/290 - 232s - loss: 0.6123 - accuracy: 0.6672 - macro_f1: 0.6853 - val_loss: 0.6136 - val_accuracy: 0.6695 - val_macro_f1: 0.6988
Epoch 22/23
290/290 - 232s - loss: 0.6085 - accuracy: 0.6719 - macro_f1: 0.6907 - val_loss: 0.6127 - val_accuracy: 0.6649 - val_macro_f1: 0.7213
Epoch 23/23
290/290 - 232s - loss: 0.6073 - accuracy: 0.6730 - macro_f1: 0.6909 - val_loss: 0.6109 - val_accuracy: 0.6715 - val_macro_f1: 0.7063
Running cyclical learning rate for Oligo_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.2.
In evaluation mode.
There 22170 positives and 18018 negatives.
41/41 - 7s
Accuracy: 0.6669554171583941. 
f1_score: 0.6711665921991723.
roc_auc: 0.7243108830727233.
prc_auc: 0.7585467603372155.
Running cyclical learning rate for Oligo_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.25.
In training mode.
There 158908 positives and 130382 negatives.
There 22170 positives and 18018 negatives.
Epoch 1/23
290/290 - 228s - loss: 0.6878 - accuracy: 0.5302 - macro_f1: 0.4982 - val_loss: 0.6820 - val_accuracy: 0.5576 - val_macro_f1: 0.5667
Epoch 2/23
290/290 - 229s - loss: 0.6766 - accuracy: 0.5691 - macro_f1: 0.5844 - val_loss: 0.6691 - val_accuracy: 0.5983 - val_macro_f1: 0.6495
Epoch 3/23
290/290 - 230s - loss: 0.6714 - accuracy: 0.5851 - macro_f1: 0.6023 - val_loss: 0.6628 - val_accuracy: 0.6097 - val_macro_f1: 0.6838
Epoch 4/23
290/290 - 231s - loss: 0.6652 - accuracy: 0.5953 - macro_f1: 0.6108 - val_loss: 0.6839 - val_accuracy: 0.5479 - val_macro_f1: 0.4208
Epoch 5/23
290/290 - 231s - loss: 0.6638 - accuracy: 0.6001 - macro_f1: 0.6146 - val_loss: 0.6623 - val_accuracy: 0.6068 - val_macro_f1: 0.6088
Epoch 6/23
290/290 - 231s - loss: 0.6585 - accuracy: 0.6087 - macro_f1: 0.6247 - val_loss: 0.6532 - val_accuracy: 0.6279 - val_macro_f1: 0.6666
Epoch 7/23
290/290 - 231s - loss: 0.6542 - accuracy: 0.6154 - macro_f1: 0.6281 - val_loss: 0.6572 - val_accuracy: 0.6187 - val_macro_f1: 0.7130
Epoch 8/23
290/290 - 231s - loss: 0.6480 - accuracy: 0.6239 - macro_f1: 0.6375 - val_loss: 0.6485 - val_accuracy: 0.6329 - val_macro_f1: 0.6420
Epoch 9/23
290/290 - 231s - loss: 0.6422 - accuracy: 0.6334 - macro_f1: 0.6484 - val_loss: 0.6565 - val_accuracy: 0.6142 - val_macro_f1: 0.5763
Epoch 10/23
290/290 - 231s - loss: 0.6389 - accuracy: 0.6356 - macro_f1: 0.6508 - val_loss: 0.6462 - val_accuracy: 0.6425 - val_macro_f1: 0.7154
Epoch 11/23
290/290 - 232s - loss: 0.6364 - accuracy: 0.6406 - macro_f1: 0.6569 - val_loss: 0.6283 - val_accuracy: 0.6533 - val_macro_f1: 0.7154
Epoch 12/23
290/290 - 232s - loss: 0.6345 - accuracy: 0.6419 - macro_f1: 0.6579 - val_loss: 0.6301 - val_accuracy: 0.6573 - val_macro_f1: 0.6944
Epoch 13/23
290/290 - 232s - loss: 0.6286 - accuracy: 0.6493 - macro_f1: 0.6680 - val_loss: 0.6301 - val_accuracy: 0.6395 - val_macro_f1: 0.7281
Epoch 14/23
290/290 - 231s - loss: 0.6283 - accuracy: 0.6486 - macro_f1: 0.6661 - val_loss: 0.6207 - val_accuracy: 0.6630 - val_macro_f1: 0.7030
Epoch 15/23
290/290 - 231s - loss: 0.6256 - accuracy: 0.6525 - macro_f1: 0.6702 - val_loss: 0.6280 - val_accuracy: 0.6559 - val_macro_f1: 0.6654
Epoch 16/23
290/290 - 232s - loss: 0.6235 - accuracy: 0.6552 - macro_f1: 0.6727 - val_loss: 0.6214 - val_accuracy: 0.6530 - val_macro_f1: 0.7270
Epoch 17/23
290/290 - 231s - loss: 0.6235 - accuracy: 0.6543 - macro_f1: 0.6716 - val_loss: 0.6367 - val_accuracy: 0.6447 - val_macro_f1: 0.6306
Epoch 18/23
290/290 - 231s - loss: 0.6228 - accuracy: 0.6553 - macro_f1: 0.6720 - val_loss: 0.6190 - val_accuracy: 0.6578 - val_macro_f1: 0.7204
Epoch 19/23
290/290 - 232s - loss: 0.6215 - accuracy: 0.6574 - macro_f1: 0.6748 - val_loss: 0.6309 - val_accuracy: 0.6549 - val_macro_f1: 0.6574
Epoch 20/23
290/290 - 231s - loss: 0.6193 - accuracy: 0.6593 - macro_f1: 0.6780 - val_loss: 0.6191 - val_accuracy: 0.6650 - val_macro_f1: 0.6877
Epoch 21/23
290/290 - 232s - loss: 0.6196 - accuracy: 0.6599 - macro_f1: 0.6758 - val_loss: 0.6471 - val_accuracy: 0.6304 - val_macro_f1: 0.5941
Epoch 22/23
290/290 - 232s - loss: 0.6222 - accuracy: 0.6564 - macro_f1: 0.6718 - val_loss: 0.6149 - val_accuracy: 0.6658 - val_macro_f1: 0.7117
Epoch 23/23
290/290 - 232s - loss: 0.6163 - accuracy: 0.6624 - macro_f1: 0.6771 - val_loss: 0.6183 - val_accuracy: 0.6674 - val_macro_f1: 0.6876
Running cyclical learning rate for Oligo_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.25.
In evaluation mode.
There 22170 positives and 18018 negatives.
41/41 - 7s
Accuracy: 0.6682059633345154. 
f1_score: 0.6682725697944188.
roc_auc: 0.72180224802011.
prc_auc: 0.755255852105704.
Running cyclical learning rate for Oligo_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.3.
In training mode.
There 158908 positives and 130382 negatives.
There 22170 positives and 18018 negatives.
Epoch 1/23
290/290 - 228s - loss: 0.6887 - accuracy: 0.5284 - macro_f1: 0.5072 - val_loss: 0.6927 - val_accuracy: 0.5138 - val_macro_f1: 0.3486
Epoch 2/23
290/290 - 229s - loss: 0.6788 - accuracy: 0.5620 - macro_f1: 0.5721 - val_loss: 0.6786 - val_accuracy: 0.5757 - val_macro_f1: 0.5614
Epoch 3/23
290/290 - 229s - loss: 0.6731 - accuracy: 0.5805 - macro_f1: 0.5970 - val_loss: 0.6833 - val_accuracy: 0.5521 - val_macro_f1: 0.4604
Epoch 4/23
290/290 - 229s - loss: 0.6678 - accuracy: 0.5928 - macro_f1: 0.6093 - val_loss: 0.6686 - val_accuracy: 0.6099 - val_macro_f1: 0.6963
Epoch 5/23
290/290 - 230s - loss: 0.6644 - accuracy: 0.5987 - macro_f1: 0.6167 - val_loss: 0.6590 - val_accuracy: 0.6147 - val_macro_f1: 0.7025
Epoch 6/23
290/290 - 230s - loss: 0.6602 - accuracy: 0.6061 - macro_f1: 0.6230 - val_loss: 0.6750 - val_accuracy: 0.5805 - val_macro_f1: 0.5043
Epoch 7/23
290/290 - 230s - loss: 0.6593 - accuracy: 0.6072 - macro_f1: 0.6202 - val_loss: 0.6768 - val_accuracy: 0.5702 - val_macro_f1: 0.4639
Epoch 8/23
290/290 - 230s - loss: 0.6510 - accuracy: 0.6195 - macro_f1: 0.6338 - val_loss: 0.6642 - val_accuracy: 0.5984 - val_macro_f1: 0.5388
Epoch 9/23
290/290 - 231s - loss: 0.6489 - accuracy: 0.6212 - macro_f1: 0.6325 - val_loss: 0.6454 - val_accuracy: 0.6400 - val_macro_f1: 0.6691
Epoch 10/23
290/290 - 231s - loss: 0.6444 - accuracy: 0.6282 - macro_f1: 0.6411 - val_loss: 0.6655 - val_accuracy: 0.6023 - val_macro_f1: 0.5470
Epoch 11/23
290/290 - 231s - loss: 0.6425 - accuracy: 0.6322 - macro_f1: 0.6471 - val_loss: 0.6550 - val_accuracy: 0.6227 - val_macro_f1: 0.5977
Epoch 12/23
290/290 - 231s - loss: 0.6387 - accuracy: 0.6362 - macro_f1: 0.6514 - val_loss: 0.6373 - val_accuracy: 0.6456 - val_macro_f1: 0.6667
Epoch 13/23
290/290 - 231s - loss: 0.6345 - accuracy: 0.6426 - macro_f1: 0.6600 - val_loss: 0.6278 - val_accuracy: 0.6570 - val_macro_f1: 0.7062
Epoch 14/23
290/290 - 231s - loss: 0.6334 - accuracy: 0.6442 - macro_f1: 0.6608 - val_loss: 0.6325 - val_accuracy: 0.6532 - val_macro_f1: 0.6650
Epoch 15/23
290/290 - 231s - loss: 0.6304 - accuracy: 0.6470 - macro_f1: 0.6641 - val_loss: 0.6348 - val_accuracy: 0.6579 - val_macro_f1: 0.6946
Epoch 16/23
290/290 - 231s - loss: 0.6301 - accuracy: 0.6472 - macro_f1: 0.6640 - val_loss: 0.6266 - val_accuracy: 0.6607 - val_macro_f1: 0.7131
Epoch 17/23
290/290 - 231s - loss: 0.6288 - accuracy: 0.6490 - macro_f1: 0.6661 - val_loss: 0.6207 - val_accuracy: 0.6617 - val_macro_f1: 0.7155
Epoch 18/23
290/290 - 231s - loss: 0.6249 - accuracy: 0.6541 - macro_f1: 0.6721 - val_loss: 0.6286 - val_accuracy: 0.6546 - val_macro_f1: 0.6652
Epoch 19/23
290/290 - 231s - loss: 0.6269 - accuracy: 0.6519 - macro_f1: 0.6686 - val_loss: 0.6583 - val_accuracy: 0.6066 - val_macro_f1: 0.5421
Epoch 20/23
290/290 - 231s - loss: 0.6248 - accuracy: 0.6541 - macro_f1: 0.6720 - val_loss: 0.6278 - val_accuracy: 0.6613 - val_macro_f1: 0.7202
Epoch 21/23
290/290 - 231s - loss: 0.6288 - accuracy: 0.6484 - macro_f1: 0.6623 - val_loss: 0.6365 - val_accuracy: 0.6463 - val_macro_f1: 0.6371
Epoch 22/23
290/290 - 231s - loss: 0.6214 - accuracy: 0.6576 - macro_f1: 0.6759 - val_loss: 0.6403 - val_accuracy: 0.6383 - val_macro_f1: 0.6109
Epoch 23/23
290/290 - 231s - loss: 0.6200 - accuracy: 0.6598 - macro_f1: 0.6775 - val_loss: 0.6215 - val_accuracy: 0.6642 - val_macro_f1: 0.6829
Running cyclical learning rate for Oligo_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.3.
In evaluation mode.
There 22170 positives and 18018 negatives.
41/41 - 7s
Accuracy: 0.6650228511527565. 
f1_score: 0.6650278140178653.
roc_auc: 0.7192499777073524.
prc_auc: 0.752023321639703.
