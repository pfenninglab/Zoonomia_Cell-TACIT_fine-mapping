Running cyclical learning rate for MSN_D1_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.05.
In training mode.
There 380766 positives and 283508 negatives.
There 48948 positives and 35420 negatives.
Epoch 1/23
665/665 - 416s - loss: 0.6656 - accuracy: 0.5904 - macro_f1: 0.6157 - val_loss: 0.6462 - val_accuracy: 0.6232 - val_macro_f1: 0.6334
Epoch 2/23
665/665 - 417s - loss: 0.6253 - accuracy: 0.6506 - macro_f1: 0.6787 - val_loss: 0.5957 - val_accuracy: 0.6932 - val_macro_f1: 0.7528
Epoch 3/23
665/665 - 416s - loss: 0.5878 - accuracy: 0.6898 - macro_f1: 0.7196 - val_loss: 0.6200 - val_accuracy: 0.6633 - val_macro_f1: 0.6558
Epoch 4/23
665/665 - 416s - loss: 0.5695 - accuracy: 0.7063 - macro_f1: 0.7360 - val_loss: 0.6504 - val_accuracy: 0.6418 - val_macro_f1: 0.6058
Epoch 5/23
665/665 - 416s - loss: 0.5588 - accuracy: 0.7153 - macro_f1: 0.7441 - val_loss: 0.5418 - val_accuracy: 0.7288 - val_macro_f1: 0.7856
Epoch 6/23
665/665 - 416s - loss: 0.5477 - accuracy: 0.7248 - macro_f1: 0.7531 - val_loss: 0.5620 - val_accuracy: 0.7116 - val_macro_f1: 0.7277
Epoch 7/23
665/665 - 416s - loss: 0.5379 - accuracy: 0.7322 - macro_f1: 0.7603 - val_loss: 0.5285 - val_accuracy: 0.7388 - val_macro_f1: 0.7980
Epoch 8/23
665/665 - 416s - loss: 0.5296 - accuracy: 0.7384 - macro_f1: 0.7659 - val_loss: 0.5221 - val_accuracy: 0.7454 - val_macro_f1: 0.7842
Epoch 9/23
665/665 - 416s - loss: 0.5224 - accuracy: 0.7436 - macro_f1: 0.7704 - val_loss: 0.5357 - val_accuracy: 0.7355 - val_macro_f1: 0.7901
Epoch 10/23
665/665 - 416s - loss: 0.5146 - accuracy: 0.7488 - macro_f1: 0.7753 - val_loss: 0.5290 - val_accuracy: 0.7397 - val_macro_f1: 0.7687
Epoch 11/23
665/665 - 416s - loss: 0.5079 - accuracy: 0.7535 - macro_f1: 0.7794 - val_loss: 0.5155 - val_accuracy: 0.7492 - val_macro_f1: 0.7967
Epoch 12/23
665/665 - 415s - loss: 0.5000 - accuracy: 0.7591 - macro_f1: 0.7848 - val_loss: 0.5285 - val_accuracy: 0.7456 - val_macro_f1: 0.7983
Epoch 13/23
665/665 - 416s - loss: 0.4918 - accuracy: 0.7636 - macro_f1: 0.7887 - val_loss: 0.5230 - val_accuracy: 0.7439 - val_macro_f1: 0.7795
Epoch 14/23
665/665 - 416s - loss: 0.4824 - accuracy: 0.7695 - macro_f1: 0.7942 - val_loss: 0.5399 - val_accuracy: 0.7374 - val_macro_f1: 0.7663
Epoch 15/23
665/665 - 416s - loss: 0.4743 - accuracy: 0.7746 - macro_f1: 0.7985 - val_loss: 0.5312 - val_accuracy: 0.7397 - val_macro_f1: 0.7769
Epoch 16/23
665/665 - 416s - loss: 0.4660 - accuracy: 0.7791 - macro_f1: 0.8026 - val_loss: 0.5359 - val_accuracy: 0.7438 - val_macro_f1: 0.7905
Epoch 17/23
665/665 - 415s - loss: 0.4599 - accuracy: 0.7832 - macro_f1: 0.8059 - val_loss: 0.5454 - val_accuracy: 0.7417 - val_macro_f1: 0.7892
Epoch 18/23
665/665 - 415s - loss: 0.4481 - accuracy: 0.7900 - macro_f1: 0.8124 - val_loss: 0.5547 - val_accuracy: 0.7260 - val_macro_f1: 0.7546
Epoch 19/23
665/665 - 415s - loss: 0.4440 - accuracy: 0.7920 - macro_f1: 0.8140 - val_loss: 0.5721 - val_accuracy: 0.7157 - val_macro_f1: 0.7357
Epoch 20/23
665/665 - 415s - loss: 0.4401 - accuracy: 0.7938 - macro_f1: 0.8154 - val_loss: 0.5782 - val_accuracy: 0.7137 - val_macro_f1: 0.7339
Epoch 21/23
665/665 - 415s - loss: 0.4279 - accuracy: 0.8015 - macro_f1: 0.8228 - val_loss: 0.5952 - val_accuracy: 0.6986 - val_macro_f1: 0.7107
Epoch 22/23
665/665 - 415s - loss: 0.4081 - accuracy: 0.8130 - macro_f1: 0.8333 - val_loss: 0.5823 - val_accuracy: 0.7278 - val_macro_f1: 0.7641
Epoch 23/23
665/665 - 415s - loss: 0.3893 - accuracy: 0.8230 - macro_f1: 0.8423 - val_loss: 0.6132 - val_accuracy: 0.7136 - val_macro_f1: 0.7422
Running cyclical learning rate for MSN_D1_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.05.
In evaluation mode.
There 48948 positives and 35420 negatives.
85/85 - 10s
Accuracy: 0.7142739604923964. 
f1_score: 0.7152419344672869.
roc_auc: 0.7879474686650492.
prc_auc: 0.8187601267902054.
Running cyclical learning rate for MSN_D1_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.1.
In training mode.
There 380766 positives and 283508 negatives.
There 48948 positives and 35420 negatives.
Epoch 1/23
665/665 - 417s - loss: 0.6699 - accuracy: 0.5824 - macro_f1: 0.6016 - val_loss: 0.6716 - val_accuracy: 0.5834 - val_macro_f1: 0.5588
Epoch 2/23
665/665 - 417s - loss: 0.6340 - accuracy: 0.6413 - macro_f1: 0.6724 - val_loss: 0.6155 - val_accuracy: 0.6660 - val_macro_f1: 0.6989
Epoch 3/23
665/665 - 416s - loss: 0.5963 - accuracy: 0.6824 - macro_f1: 0.7129 - val_loss: 0.5744 - val_accuracy: 0.7026 - val_macro_f1: 0.7417
Epoch 4/23
665/665 - 416s - loss: 0.5793 - accuracy: 0.6978 - macro_f1: 0.7274 - val_loss: 0.6174 - val_accuracy: 0.6656 - val_macro_f1: 0.6540
Epoch 5/23
665/665 - 416s - loss: 0.5639 - accuracy: 0.7112 - macro_f1: 0.7409 - val_loss: 0.5398 - val_accuracy: 0.7291 - val_macro_f1: 0.7815
Epoch 6/23
665/665 - 416s - loss: 0.5529 - accuracy: 0.7207 - macro_f1: 0.7495 - val_loss: 0.5443 - val_accuracy: 0.7281 - val_macro_f1: 0.7683
Epoch 7/23
665/665 - 416s - loss: 0.5455 - accuracy: 0.7269 - macro_f1: 0.7552 - val_loss: 0.5517 - val_accuracy: 0.7197 - val_macro_f1: 0.7365
Epoch 8/23
665/665 - 416s - loss: 0.5387 - accuracy: 0.7316 - macro_f1: 0.7595 - val_loss: 0.5284 - val_accuracy: 0.7393 - val_macro_f1: 0.7813
Epoch 9/23
665/665 - 416s - loss: 0.5312 - accuracy: 0.7382 - macro_f1: 0.7657 - val_loss: 0.5220 - val_accuracy: 0.7431 - val_macro_f1: 0.7826
Epoch 10/23
665/665 - 416s - loss: 0.5252 - accuracy: 0.7415 - macro_f1: 0.7689 - val_loss: 0.5228 - val_accuracy: 0.7461 - val_macro_f1: 0.7962
Epoch 11/23
665/665 - 416s - loss: 0.5217 - accuracy: 0.7439 - macro_f1: 0.7710 - val_loss: 0.5563 - val_accuracy: 0.7187 - val_macro_f1: 0.7320
Epoch 12/23
665/665 - 415s - loss: 0.5158 - accuracy: 0.7478 - macro_f1: 0.7743 - val_loss: 0.5148 - val_accuracy: 0.7508 - val_macro_f1: 0.7908
Epoch 13/23
665/665 - 416s - loss: 0.5076 - accuracy: 0.7540 - macro_f1: 0.7807 - val_loss: 0.5152 - val_accuracy: 0.7491 - val_macro_f1: 0.7860
Epoch 14/23
665/665 - 416s - loss: 0.5042 - accuracy: 0.7558 - macro_f1: 0.7818 - val_loss: 0.5202 - val_accuracy: 0.7462 - val_macro_f1: 0.7805
Epoch 15/23
665/665 - 416s - loss: 0.4973 - accuracy: 0.7603 - macro_f1: 0.7864 - val_loss: 0.5236 - val_accuracy: 0.7434 - val_macro_f1: 0.7737
Epoch 16/23
665/665 - 416s - loss: 0.4943 - accuracy: 0.7619 - macro_f1: 0.7872 - val_loss: 0.5431 - val_accuracy: 0.7316 - val_macro_f1: 0.7530
Epoch 17/23
665/665 - 416s - loss: 0.4872 - accuracy: 0.7666 - macro_f1: 0.7920 - val_loss: 0.5267 - val_accuracy: 0.7398 - val_macro_f1: 0.7733
Epoch 18/23
665/665 - 415s - loss: 0.4815 - accuracy: 0.7700 - macro_f1: 0.7950 - val_loss: 0.5534 - val_accuracy: 0.7209 - val_macro_f1: 0.7376
Epoch 19/23
665/665 - 415s - loss: 0.4775 - accuracy: 0.7728 - macro_f1: 0.7974 - val_loss: 0.5217 - val_accuracy: 0.7485 - val_macro_f1: 0.7934
Epoch 20/23
665/665 - 415s - loss: 0.4756 - accuracy: 0.7732 - macro_f1: 0.7978 - val_loss: 0.5349 - val_accuracy: 0.7389 - val_macro_f1: 0.7690
Epoch 21/23
665/665 - 415s - loss: 0.4736 - accuracy: 0.7749 - macro_f1: 0.7987 - val_loss: 0.5324 - val_accuracy: 0.7451 - val_macro_f1: 0.7828
Epoch 22/23
665/665 - 415s - loss: 0.4629 - accuracy: 0.7815 - macro_f1: 0.8047 - val_loss: 0.5539 - val_accuracy: 0.7288 - val_macro_f1: 0.7525
Epoch 23/23
665/665 - 415s - loss: 0.4535 - accuracy: 0.7865 - macro_f1: 0.8090 - val_loss: 0.5360 - val_accuracy: 0.7398 - val_macro_f1: 0.7752
Running cyclical learning rate for MSN_D1_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.1.
In evaluation mode.
There 48948 positives and 35420 negatives.
85/85 - 10s
Accuracy: 0.7331675089853245. 
f1_score: 0.7398826217839204.
roc_auc: 0.8090664131774086.
prc_auc: 0.837097427292065.
Running cyclical learning rate for MSN_D1_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.15.
In training mode.
There 380766 positives and 283508 negatives.
There 48948 positives and 35420 negatives.
Epoch 1/23
665/665 - 417s - loss: 0.6714 - accuracy: 0.5740 - macro_f1: 0.5836 - val_loss: 0.6629 - val_accuracy: 0.6004 - val_macro_f1: 0.6083
Epoch 2/23
665/665 - 417s - loss: 0.6316 - accuracy: 0.6438 - macro_f1: 0.6759 - val_loss: 0.6712 - val_accuracy: 0.6058 - val_macro_f1: 0.5611
Epoch 3/23
665/665 - 415s - loss: 0.5970 - accuracy: 0.6812 - macro_f1: 0.7113 - val_loss: 0.5858 - val_accuracy: 0.6952 - val_macro_f1: 0.7176
Epoch 4/23
665/665 - 415s - loss: 0.5790 - accuracy: 0.6982 - macro_f1: 0.7282 - val_loss: 0.5662 - val_accuracy: 0.7123 - val_macro_f1: 0.7445
Epoch 5/23
665/665 - 415s - loss: 0.5704 - accuracy: 0.7056 - macro_f1: 0.7351 - val_loss: 0.5585 - val_accuracy: 0.7253 - val_macro_f1: 0.7834
Epoch 6/23
665/665 - 414s - loss: 0.5616 - accuracy: 0.7137 - macro_f1: 0.7429 - val_loss: 0.5464 - val_accuracy: 0.7319 - val_macro_f1: 0.7785
Epoch 7/23
665/665 - 414s - loss: 0.5548 - accuracy: 0.7192 - macro_f1: 0.7481 - val_loss: 0.5401 - val_accuracy: 0.7314 - val_macro_f1: 0.7639
Epoch 8/23
665/665 - 414s - loss: 0.5475 - accuracy: 0.7252 - macro_f1: 0.7536 - val_loss: 0.6143 - val_accuracy: 0.6674 - val_macro_f1: 0.6453
Epoch 9/23
665/665 - 414s - loss: 0.5421 - accuracy: 0.7291 - macro_f1: 0.7571 - val_loss: 0.5266 - val_accuracy: 0.7426 - val_macro_f1: 0.7890
Epoch 10/23
665/665 - 414s - loss: 0.5376 - accuracy: 0.7328 - macro_f1: 0.7606 - val_loss: 0.5378 - val_accuracy: 0.7333 - val_macro_f1: 0.7570
Epoch 11/23
665/665 - 414s - loss: 0.5321 - accuracy: 0.7363 - macro_f1: 0.7643 - val_loss: 0.5215 - val_accuracy: 0.7466 - val_macro_f1: 0.7813
Epoch 12/23
665/665 - 414s - loss: 0.5281 - accuracy: 0.7394 - macro_f1: 0.7668 - val_loss: 0.5717 - val_accuracy: 0.7012 - val_macro_f1: 0.6992
Epoch 13/23
665/665 - 414s - loss: 0.5246 - accuracy: 0.7422 - macro_f1: 0.7696 - val_loss: 0.5165 - val_accuracy: 0.7465 - val_macro_f1: 0.7854
Epoch 14/23
665/665 - 414s - loss: 0.5190 - accuracy: 0.7457 - macro_f1: 0.7731 - val_loss: 0.5418 - val_accuracy: 0.7292 - val_macro_f1: 0.7461
Epoch 15/23
665/665 - 414s - loss: 0.5159 - accuracy: 0.7483 - macro_f1: 0.7752 - val_loss: 0.5126 - val_accuracy: 0.7511 - val_macro_f1: 0.7936
Epoch 16/23
665/665 - 414s - loss: 0.5113 - accuracy: 0.7512 - macro_f1: 0.7782 - val_loss: 0.5185 - val_accuracy: 0.7433 - val_macro_f1: 0.7742
Epoch 17/23
665/665 - 414s - loss: 0.5077 - accuracy: 0.7539 - macro_f1: 0.7806 - val_loss: 0.5177 - val_accuracy: 0.7481 - val_macro_f1: 0.7807
Epoch 18/23
665/665 - 414s - loss: 0.5040 - accuracy: 0.7560 - macro_f1: 0.7823 - val_loss: 0.5171 - val_accuracy: 0.7476 - val_macro_f1: 0.7802
Epoch 19/23
665/665 - 414s - loss: 0.5009 - accuracy: 0.7582 - macro_f1: 0.7846 - val_loss: 0.5579 - val_accuracy: 0.7129 - val_macro_f1: 0.7203
Epoch 20/23
665/665 - 413s - loss: 0.5044 - accuracy: 0.7556 - macro_f1: 0.7815 - val_loss: 0.5383 - val_accuracy: 0.7312 - val_macro_f1: 0.7507
Epoch 21/23
665/665 - 413s - loss: 0.5011 - accuracy: 0.7579 - macro_f1: 0.7833 - val_loss: 0.5560 - val_accuracy: 0.7201 - val_macro_f1: 0.7338
Epoch 22/23
665/665 - 413s - loss: 0.4935 - accuracy: 0.7627 - macro_f1: 0.7879 - val_loss: 0.5259 - val_accuracy: 0.7392 - val_macro_f1: 0.7661
Epoch 23/23
665/665 - 413s - loss: 0.4879 - accuracy: 0.7658 - macro_f1: 0.7906 - val_loss: 0.5689 - val_accuracy: 0.7082 - val_macro_f1: 0.7124
Running cyclical learning rate for MSN_D1_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.15.
In evaluation mode.
There 48948 positives and 35420 negatives.
85/85 - 10s
Accuracy: 0.7245739102841227. 
f1_score: 0.7088530626179443.
roc_auc: 0.8153109143655234.
prc_auc: 0.8445339890794055.
Running cyclical learning rate for MSN_D1_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.2.
In training mode.
There 380766 positives and 283508 negatives.
There 48948 positives and 35420 negatives.
Epoch 1/23
665/665 - 414s - loss: 0.6719 - accuracy: 0.5751 - macro_f1: 0.5868 - val_loss: 0.6549 - val_accuracy: 0.6166 - val_macro_f1: 0.6487
Epoch 2/23
665/665 - 413s - loss: 0.6372 - accuracy: 0.6373 - macro_f1: 0.6695 - val_loss: 0.6217 - val_accuracy: 0.6533 - val_macro_f1: 0.6795
Epoch 3/23
665/665 - 414s - loss: 0.6072 - accuracy: 0.6708 - macro_f1: 0.7005 - val_loss: 0.6229 - val_accuracy: 0.6558 - val_macro_f1: 0.6477
Epoch 4/23
665/665 - 413s - loss: 0.5838 - accuracy: 0.6938 - macro_f1: 0.7248 - val_loss: 0.6031 - val_accuracy: 0.6735 - val_macro_f1: 0.6748
Epoch 5/23
665/665 - 412s - loss: 0.5769 - accuracy: 0.7000 - macro_f1: 0.7293 - val_loss: 0.5636 - val_accuracy: 0.7164 - val_macro_f1: 0.7489
Epoch 6/23
665/665 - 412s - loss: 0.5673 - accuracy: 0.7090 - macro_f1: 0.7386 - val_loss: 0.5804 - val_accuracy: 0.6987 - val_macro_f1: 0.7083
Epoch 7/23
665/665 - 412s - loss: 0.5609 - accuracy: 0.7145 - macro_f1: 0.7432 - val_loss: 0.5547 - val_accuracy: 0.7207 - val_macro_f1: 0.7483
Epoch 8/23
665/665 - 412s - loss: 0.5544 - accuracy: 0.7199 - macro_f1: 0.7488 - val_loss: 0.5820 - val_accuracy: 0.6927 - val_macro_f1: 0.6908
Epoch 9/23
665/665 - 412s - loss: 0.5492 - accuracy: 0.7243 - macro_f1: 0.7525 - val_loss: 0.5510 - val_accuracy: 0.7210 - val_macro_f1: 0.7400
Epoch 10/23
665/665 - 412s - loss: 0.5448 - accuracy: 0.7272 - macro_f1: 0.7553 - val_loss: 0.5827 - val_accuracy: 0.6960 - val_macro_f1: 0.6935
Epoch 11/23
665/665 - 412s - loss: 0.5405 - accuracy: 0.7303 - macro_f1: 0.7584 - val_loss: 0.5225 - val_accuracy: 0.7456 - val_macro_f1: 0.7809
Epoch 12/23
665/665 - 411s - loss: 0.5335 - accuracy: 0.7353 - macro_f1: 0.7637 - val_loss: 0.5185 - val_accuracy: 0.7485 - val_macro_f1: 0.7922
Epoch 13/23
665/665 - 411s - loss: 0.5317 - accuracy: 0.7369 - macro_f1: 0.7646 - val_loss: 0.5219 - val_accuracy: 0.7477 - val_macro_f1: 0.7910
Epoch 14/23
665/665 - 411s - loss: 0.5275 - accuracy: 0.7397 - macro_f1: 0.7676 - val_loss: 0.5365 - val_accuracy: 0.7340 - val_macro_f1: 0.7558
Epoch 15/23
665/665 - 411s - loss: 0.5248 - accuracy: 0.7417 - macro_f1: 0.7694 - val_loss: 0.5123 - val_accuracy: 0.7510 - val_macro_f1: 0.7982
Epoch 16/23
665/665 - 411s - loss: 0.5208 - accuracy: 0.7449 - macro_f1: 0.7723 - val_loss: 0.5182 - val_accuracy: 0.7472 - val_macro_f1: 0.7768
Epoch 17/23
665/665 - 411s - loss: 0.5194 - accuracy: 0.7456 - macro_f1: 0.7728 - val_loss: 0.5553 - val_accuracy: 0.7178 - val_macro_f1: 0.7251
Epoch 18/23
665/665 - 411s - loss: 0.5156 - accuracy: 0.7481 - macro_f1: 0.7753 - val_loss: 0.5544 - val_accuracy: 0.7213 - val_macro_f1: 0.7310
Epoch 19/23
665/665 - 411s - loss: 0.5137 - accuracy: 0.7497 - macro_f1: 0.7766 - val_loss: 0.5282 - val_accuracy: 0.7430 - val_macro_f1: 0.7691
Epoch 20/23
665/665 - 411s - loss: 0.5097 - accuracy: 0.7524 - macro_f1: 0.7796 - val_loss: 0.5553 - val_accuracy: 0.7190 - val_macro_f1: 0.7283
Epoch 21/23
665/665 - 411s - loss: 0.5103 - accuracy: 0.7519 - macro_f1: 0.7784 - val_loss: 0.5114 - val_accuracy: 0.7519 - val_macro_f1: 0.7846
Epoch 22/23
665/665 - 412s - loss: 0.5044 - accuracy: 0.7555 - macro_f1: 0.7822 - val_loss: 0.5526 - val_accuracy: 0.7208 - val_macro_f1: 0.7289
Epoch 23/23
665/665 - 413s - loss: 0.4992 - accuracy: 0.7590 - macro_f1: 0.7851 - val_loss: 0.5111 - val_accuracy: 0.7530 - val_macro_f1: 0.7850
  File "train_singleTask_CNN_classifier_OCP.py", line 152
    break
    ^
SyntaxError: 'break' outside loop
  File "train_singleTask_CNN_classifier_OCP.py", line 152
    break
    ^
SyntaxError: 'break' outside loop
  File "train_singleTask_CNN_classifier_OCP.py", line 152
    break
    ^
SyntaxError: 'break' outside loop
  File "train_singleTask_CNN_classifier_OCP.py", line 152
    break
    ^
SyntaxError: 'break' outside loop
  File "train_singleTask_CNN_classifier_OCP.py", line 152
    break
    ^
SyntaxError: 'break' outside loop
