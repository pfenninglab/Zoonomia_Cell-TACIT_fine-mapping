Running cyclical learning rate for MSN_D2_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.05.
In training mode.
There 396702 positives and 286498 negatives.
There 49808 positives and 35642 negatives.
Epoch 1/23
684/684 - 429s - loss: 0.6646 - accuracy: 0.5909 - macro_f1: 0.6123 - val_loss: 0.6472 - val_accuracy: 0.6250 - val_macro_f1: 0.6465
Epoch 2/23
684/684 - 430s - loss: 0.6159 - accuracy: 0.6624 - macro_f1: 0.6948 - val_loss: 0.5904 - val_accuracy: 0.6863 - val_macro_f1: 0.7148
Epoch 3/23
684/684 - 430s - loss: 0.5799 - accuracy: 0.6983 - macro_f1: 0.7311 - val_loss: 0.5545 - val_accuracy: 0.7176 - val_macro_f1: 0.7581
Epoch 4/23
684/684 - 430s - loss: 0.5615 - accuracy: 0.7140 - macro_f1: 0.7462 - val_loss: 0.5905 - val_accuracy: 0.6944 - val_macro_f1: 0.7010
Epoch 5/23
684/684 - 430s - loss: 0.5509 - accuracy: 0.7222 - macro_f1: 0.7538 - val_loss: 0.5366 - val_accuracy: 0.7341 - val_macro_f1: 0.7686
Epoch 6/23
684/684 - 430s - loss: 0.5407 - accuracy: 0.7298 - macro_f1: 0.7606 - val_loss: 0.5311 - val_accuracy: 0.7372 - val_macro_f1: 0.7698
Epoch 7/23
684/684 - 430s - loss: 0.5310 - accuracy: 0.7376 - macro_f1: 0.7677 - val_loss: 0.5185 - val_accuracy: 0.7469 - val_macro_f1: 0.7933
Epoch 8/23
684/684 - 430s - loss: 0.5223 - accuracy: 0.7442 - macro_f1: 0.7738 - val_loss: 0.5206 - val_accuracy: 0.7463 - val_macro_f1: 0.7807
Epoch 9/23
684/684 - 430s - loss: 0.5162 - accuracy: 0.7476 - macro_f1: 0.7768 - val_loss: 0.5205 - val_accuracy: 0.7437 - val_macro_f1: 0.7740
Epoch 10/23
684/684 - 430s - loss: 0.5078 - accuracy: 0.7539 - macro_f1: 0.7825 - val_loss: 0.5603 - val_accuracy: 0.7192 - val_macro_f1: 0.7317
Epoch 11/23
684/684 - 430s - loss: 0.5014 - accuracy: 0.7580 - macro_f1: 0.7858 - val_loss: 0.5270 - val_accuracy: 0.7435 - val_macro_f1: 0.7730
Epoch 12/23
684/684 - 430s - loss: 0.4911 - accuracy: 0.7647 - macro_f1: 0.7924 - val_loss: 0.5196 - val_accuracy: 0.7444 - val_macro_f1: 0.7770
Epoch 13/23
684/684 - 430s - loss: 0.4847 - accuracy: 0.7687 - macro_f1: 0.7957 - val_loss: 0.5373 - val_accuracy: 0.7334 - val_macro_f1: 0.7582
Epoch 14/23
684/684 - 430s - loss: 0.4769 - accuracy: 0.7729 - macro_f1: 0.7991 - val_loss: 0.5218 - val_accuracy: 0.7458 - val_macro_f1: 0.7777
Epoch 15/23
684/684 - 430s - loss: 0.4671 - accuracy: 0.7792 - macro_f1: 0.8052 - val_loss: 0.5225 - val_accuracy: 0.7475 - val_macro_f1: 0.7900
Epoch 16/23
684/684 - 430s - loss: 0.4588 - accuracy: 0.7843 - macro_f1: 0.8097 - val_loss: 0.5323 - val_accuracy: 0.7382 - val_macro_f1: 0.7706
Epoch 17/23
684/684 - 430s - loss: 0.4500 - accuracy: 0.7896 - macro_f1: 0.8143 - val_loss: 0.5449 - val_accuracy: 0.7364 - val_macro_f1: 0.7661
Epoch 18/23
684/684 - 430s - loss: 0.4406 - accuracy: 0.7949 - macro_f1: 0.8192 - val_loss: 0.5650 - val_accuracy: 0.7233 - val_macro_f1: 0.7439
Epoch 19/23
684/684 - 430s - loss: 0.4353 - accuracy: 0.7979 - macro_f1: 0.8216 - val_loss: 0.5406 - val_accuracy: 0.7378 - val_macro_f1: 0.7730
Epoch 20/23
684/684 - 430s - loss: 0.4299 - accuracy: 0.8006 - macro_f1: 0.8242 - val_loss: 0.5541 - val_accuracy: 0.7343 - val_macro_f1: 0.7692
Epoch 21/23
684/684 - 430s - loss: 0.4155 - accuracy: 0.8090 - macro_f1: 0.8318 - val_loss: 0.5615 - val_accuracy: 0.7303 - val_macro_f1: 0.7636
Epoch 22/23
684/684 - 430s - loss: 0.4066 - accuracy: 0.8131 - macro_f1: 0.8350 - val_loss: 0.5695 - val_accuracy: 0.7293 - val_macro_f1: 0.7646
Epoch 23/23
684/684 - 427s - loss: 0.3827 - accuracy: 0.8269 - macro_f1: 0.8476 - val_loss: 0.5788 - val_accuracy: 0.7326 - val_macro_f1: 0.7714
Running cyclical learning rate for MSN_D2_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.05.
In evaluation mode.
There 49808 positives and 35642 negatives.
86/86 - 11s
Accuracy: 0.7242570457144291. 
f1_score: 0.7323613908391107.
roc_auc: 0.7993813594519998.
prc_auc: 0.8318078081289909.
Running cyclical learning rate for MSN_D2_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.1.
In training mode.
There 396702 positives and 286498 negatives.
There 49808 positives and 35642 negatives.
Epoch 1/23
684/684 - 429s - loss: 0.6672 - accuracy: 0.5893 - macro_f1: 0.6196 - val_loss: 0.6549 - val_accuracy: 0.6167 - val_macro_f1: 0.6324
Epoch 2/23
684/684 - 430s - loss: 0.6252 - accuracy: 0.6522 - macro_f1: 0.6861 - val_loss: 0.6128 - val_accuracy: 0.6680 - val_macro_f1: 0.6921
Epoch 3/23
684/684 - 430s - loss: 0.5887 - accuracy: 0.6895 - macro_f1: 0.7218 - val_loss: 0.6280 - val_accuracy: 0.6363 - val_macro_f1: 0.6018
Epoch 4/23
684/684 - 430s - loss: 0.5687 - accuracy: 0.7075 - macro_f1: 0.7401 - val_loss: 0.5457 - val_accuracy: 0.7266 - val_macro_f1: 0.7897
Epoch 5/23
684/684 - 430s - loss: 0.5587 - accuracy: 0.7160 - macro_f1: 0.7476 - val_loss: 0.5376 - val_accuracy: 0.7342 - val_macro_f1: 0.7745
Epoch 6/23
684/684 - 430s - loss: 0.5487 - accuracy: 0.7241 - macro_f1: 0.7553 - val_loss: 0.5334 - val_accuracy: 0.7371 - val_macro_f1: 0.7969
Epoch 7/23
684/684 - 430s - loss: 0.5406 - accuracy: 0.7301 - macro_f1: 0.7606 - val_loss: 0.5647 - val_accuracy: 0.7120 - val_macro_f1: 0.7234
Epoch 8/23
684/684 - 430s - loss: 0.5353 - accuracy: 0.7341 - macro_f1: 0.7645 - val_loss: 0.5349 - val_accuracy: 0.7365 - val_macro_f1: 0.7640
Epoch 9/23
684/684 - 430s - loss: 0.5278 - accuracy: 0.7403 - macro_f1: 0.7702 - val_loss: 0.5219 - val_accuracy: 0.7454 - val_macro_f1: 0.7837
Epoch 10/23
684/684 - 430s - loss: 0.5236 - accuracy: 0.7427 - macro_f1: 0.7718 - val_loss: 0.5303 - val_accuracy: 0.7377 - val_macro_f1: 0.7626
Epoch 11/23
684/684 - 430s - loss: 0.5177 - accuracy: 0.7468 - macro_f1: 0.7762 - val_loss: 0.5769 - val_accuracy: 0.7051 - val_macro_f1: 0.7055
Epoch 12/23
684/684 - 430s - loss: 0.5121 - accuracy: 0.7508 - macro_f1: 0.7798 - val_loss: 0.5112 - val_accuracy: 0.7514 - val_macro_f1: 0.7865
Epoch 13/23
684/684 - 430s - loss: 0.5045 - accuracy: 0.7559 - macro_f1: 0.7846 - val_loss: 0.5089 - val_accuracy: 0.7530 - val_macro_f1: 0.7884
Epoch 14/23
684/684 - 430s - loss: 0.5006 - accuracy: 0.7586 - macro_f1: 0.7867 - val_loss: 0.5093 - val_accuracy: 0.7547 - val_macro_f1: 0.8005
Epoch 15/23
684/684 - 430s - loss: 0.4944 - accuracy: 0.7621 - macro_f1: 0.7902 - val_loss: 0.5211 - val_accuracy: 0.7423 - val_macro_f1: 0.7703
Epoch 16/23
684/684 - 430s - loss: 0.4881 - accuracy: 0.7661 - macro_f1: 0.7937 - val_loss: 0.5315 - val_accuracy: 0.7370 - val_macro_f1: 0.7590
Epoch 17/23
684/684 - 430s - loss: 0.4850 - accuracy: 0.7682 - macro_f1: 0.7952 - val_loss: 0.5435 - val_accuracy: 0.7280 - val_macro_f1: 0.7459
Epoch 18/23
684/684 - 430s - loss: 0.4814 - accuracy: 0.7705 - macro_f1: 0.7971 - val_loss: 0.5141 - val_accuracy: 0.7480 - val_macro_f1: 0.7840
Epoch 19/23
684/684 - 430s - loss: 0.4752 - accuracy: 0.7745 - macro_f1: 0.8009 - val_loss: 0.5606 - val_accuracy: 0.7146 - val_macro_f1: 0.7240
Epoch 20/23
684/684 - 430s - loss: 0.4775 - accuracy: 0.7725 - macro_f1: 0.7989 - val_loss: 0.5412 - val_accuracy: 0.7321 - val_macro_f1: 0.7555
Epoch 21/23
684/684 - 430s - loss: 0.4685 - accuracy: 0.7778 - macro_f1: 0.8036 - val_loss: 0.5256 - val_accuracy: 0.7415 - val_macro_f1: 0.7727
Epoch 22/23
684/684 - 428s - loss: 0.4577 - accuracy: 0.7849 - macro_f1: 0.8102 - val_loss: 0.5215 - val_accuracy: 0.7497 - val_macro_f1: 0.7940
Epoch 23/23
684/684 - 425s - loss: 0.4495 - accuracy: 0.7896 - macro_f1: 0.8142 - val_loss: 0.5270 - val_accuracy: 0.7449 - val_macro_f1: 0.7798
Running cyclical learning rate for MSN_D2_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.1.
In evaluation mode.
There 49808 positives and 35642 negatives.
86/86 - 11s
Accuracy: 0.7386578884103443. 
f1_score: 0.7451315616920543.
roc_auc: 0.8165228164496878.
prc_auc: 0.8454782798896232.
Running cyclical learning rate for MSN_D2_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.15.
In training mode.
There 396702 positives and 286498 negatives.
There 49808 positives and 35642 negatives.
Epoch 1/23
684/684 - 429s - loss: 0.6676 - accuracy: 0.5861 - macro_f1: 0.6097 - val_loss: 0.6522 - val_accuracy: 0.6179 - val_macro_f1: 0.6265
Epoch 2/23
684/684 - 430s - loss: 0.6281 - accuracy: 0.6489 - macro_f1: 0.6817 - val_loss: 0.6913 - val_accuracy: 0.5523 - val_macro_f1: 0.4310
Epoch 3/23
684/684 - 430s - loss: 0.5943 - accuracy: 0.6843 - macro_f1: 0.7171 - val_loss: 0.5571 - val_accuracy: 0.7211 - val_macro_f1: 0.7739
Epoch 4/23
684/684 - 430s - loss: 0.5723 - accuracy: 0.7043 - macro_f1: 0.7368 - val_loss: 0.5659 - val_accuracy: 0.7114 - val_macro_f1: 0.7348
Epoch 5/23
684/684 - 430s - loss: 0.5609 - accuracy: 0.7140 - macro_f1: 0.7460 - val_loss: 0.5488 - val_accuracy: 0.7198 - val_macro_f1: 0.7930
Epoch 6/23
684/684 - 430s - loss: 0.5544 - accuracy: 0.7193 - macro_f1: 0.7508 - val_loss: 0.5783 - val_accuracy: 0.7003 - val_macro_f1: 0.7056
Epoch 7/23
684/684 - 430s - loss: 0.5455 - accuracy: 0.7269 - macro_f1: 0.7577 - val_loss: 0.6231 - val_accuracy: 0.6586 - val_macro_f1: 0.6278
Epoch 8/23
684/684 - 430s - loss: 0.5396 - accuracy: 0.7310 - macro_f1: 0.7612 - val_loss: 0.5361 - val_accuracy: 0.7357 - val_macro_f1: 0.7655
Epoch 9/23
684/684 - 430s - loss: 0.5326 - accuracy: 0.7359 - macro_f1: 0.7661 - val_loss: 0.5798 - val_accuracy: 0.6964 - val_macro_f1: 0.6930
Epoch 10/23
684/684 - 430s - loss: 0.5276 - accuracy: 0.7402 - macro_f1: 0.7700 - val_loss: 0.5091 - val_accuracy: 0.7522 - val_macro_f1: 0.8011
Epoch 11/23
684/684 - 430s - loss: 0.5232 - accuracy: 0.7426 - macro_f1: 0.7722 - val_loss: 0.5142 - val_accuracy: 0.7501 - val_macro_f1: 0.7824
Epoch 12/23
684/684 - 430s - loss: 0.5185 - accuracy: 0.7460 - macro_f1: 0.7753 - val_loss: 0.5152 - val_accuracy: 0.7481 - val_macro_f1: 0.7786
Epoch 13/23
684/684 - 430s - loss: 0.5131 - accuracy: 0.7498 - macro_f1: 0.7789 - val_loss: 0.5297 - val_accuracy: 0.7377 - val_macro_f1: 0.7585
Epoch 14/23
684/684 - 430s - loss: 0.5092 - accuracy: 0.7524 - macro_f1: 0.7814 - val_loss: 0.5104 - val_accuracy: 0.7528 - val_macro_f1: 0.7827
Epoch 15/23
684/684 - 430s - loss: 0.5056 - accuracy: 0.7552 - macro_f1: 0.7839 - val_loss: 0.5156 - val_accuracy: 0.7481 - val_macro_f1: 0.7749
Epoch 16/23
684/684 - 430s - loss: 0.5018 - accuracy: 0.7575 - macro_f1: 0.7857 - val_loss: 0.5327 - val_accuracy: 0.7298 - val_macro_f1: 0.7502
Epoch 17/23
684/684 - 430s - loss: 0.4972 - accuracy: 0.7604 - macro_f1: 0.7885 - val_loss: 0.5495 - val_accuracy: 0.7220 - val_macro_f1: 0.7326
Epoch 18/23
684/684 - 430s - loss: 0.4949 - accuracy: 0.7617 - macro_f1: 0.7895 - val_loss: 0.5196 - val_accuracy: 0.7424 - val_macro_f1: 0.7681
Epoch 19/23
684/684 - 430s - loss: 0.4900 - accuracy: 0.7649 - macro_f1: 0.7926 - val_loss: 0.5136 - val_accuracy: 0.7478 - val_macro_f1: 0.7782
Epoch 20/23
684/684 - 430s - loss: 0.4916 - accuracy: 0.7640 - macro_f1: 0.7913 - val_loss: 0.5234 - val_accuracy: 0.7398 - val_macro_f1: 0.7633
Epoch 21/23
684/684 - 428s - loss: 0.4859 - accuracy: 0.7673 - macro_f1: 0.7944 - val_loss: 0.5648 - val_accuracy: 0.7135 - val_macro_f1: 0.7202
Epoch 22/23
684/684 - 424s - loss: 0.4838 - accuracy: 0.7686 - macro_f1: 0.7952 - val_loss: 0.5664 - val_accuracy: 0.7112 - val_macro_f1: 0.7155
Epoch 23/23
684/684 - 430s - loss: 0.4736 - accuracy: 0.7747 - macro_f1: 0.8013 - val_loss: 0.5119 - val_accuracy: 0.7495 - val_macro_f1: 0.7816
Running cyclical learning rate for MSN_D2_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.15.
In evaluation mode.
There 49808 positives and 35642 negatives.
86/86 - 11s
Accuracy: 0.7454539741568962. 
f1_score: 0.7501951350322823.
roc_auc: 0.8248696671893658.
prc_auc: 0.8547370713005809.
Running cyclical learning rate for MSN_D2_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.2.
In training mode.
There 396702 positives and 286498 negatives.
There 49808 positives and 35642 negatives.
Epoch 1/23
684/684 - 429s - loss: 0.6691 - accuracy: 0.5836 - macro_f1: 0.6093 - val_loss: 0.6394 - val_accuracy: 0.6530 - val_macro_f1: 0.7386
Epoch 2/23
684/684 - 430s - loss: 0.6250 - accuracy: 0.6535 - macro_f1: 0.6883 - val_loss: 0.6077 - val_accuracy: 0.6828 - val_macro_f1: 0.7366
Epoch 3/23
684/684 - 430s - loss: 0.5964 - accuracy: 0.6835 - macro_f1: 0.7176 - val_loss: 0.5725 - val_accuracy: 0.7156 - val_macro_f1: 0.7659
Epoch 4/23
684/684 - 430s - loss: 0.5755 - accuracy: 0.7020 - macro_f1: 0.7353 - val_loss: 0.5574 - val_accuracy: 0.7172 - val_macro_f1: 0.7563
Epoch 5/23
684/684 - 430s - loss: 0.5650 - accuracy: 0.7109 - macro_f1: 0.7434 - val_loss: 0.6035 - val_accuracy: 0.6778 - val_macro_f1: 0.6736
Epoch 6/23
684/684 - 430s - loss: 0.5593 - accuracy: 0.7154 - macro_f1: 0.7469 - val_loss: 0.5402 - val_accuracy: 0.7345 - val_macro_f1: 0.7782
Epoch 7/23
684/684 - 430s - loss: 0.5503 - accuracy: 0.7230 - macro_f1: 0.7546 - val_loss: 0.6197 - val_accuracy: 0.6500 - val_macro_f1: 0.6140
Epoch 8/23
684/684 - 430s - loss: 0.5450 - accuracy: 0.7266 - macro_f1: 0.7578 - val_loss: 0.5221 - val_accuracy: 0.7435 - val_macro_f1: 0.7983
Epoch 9/23
684/684 - 430s - loss: 0.5411 - accuracy: 0.7298 - macro_f1: 0.7605 - val_loss: 0.6543 - val_accuracy: 0.6299 - val_macro_f1: 0.5703
Epoch 10/23
684/684 - 430s - loss: 0.5366 - accuracy: 0.7332 - macro_f1: 0.7636 - val_loss: 0.5204 - val_accuracy: 0.7482 - val_macro_f1: 0.7826
Epoch 11/23
684/684 - 430s - loss: 0.5317 - accuracy: 0.7366 - macro_f1: 0.7668 - val_loss: 0.5260 - val_accuracy: 0.7384 - val_macro_f1: 0.8023
Epoch 12/23
684/684 - 430s - loss: 0.5271 - accuracy: 0.7402 - macro_f1: 0.7705 - val_loss: 0.5257 - val_accuracy: 0.7418 - val_macro_f1: 0.7657
Epoch 13/23
684/684 - 430s - loss: 0.5230 - accuracy: 0.7440 - macro_f1: 0.7739 - val_loss: 0.5147 - val_accuracy: 0.7508 - val_macro_f1: 0.7821
Epoch 14/23
684/684 - 430s - loss: 0.5195 - accuracy: 0.7456 - macro_f1: 0.7752 - val_loss: 0.5669 - val_accuracy: 0.7042 - val_macro_f1: 0.7032
Epoch 15/23
684/684 - 430s - loss: 0.5152 - accuracy: 0.7486 - macro_f1: 0.7781 - val_loss: 0.5302 - val_accuracy: 0.7402 - val_macro_f1: 0.7618
Epoch 16/23
684/684 - 430s - loss: 0.5108 - accuracy: 0.7518 - macro_f1: 0.7812 - val_loss: 0.4996 - val_accuracy: 0.7580 - val_macro_f1: 0.8034
Epoch 17/23
684/684 - 430s - loss: 0.5068 - accuracy: 0.7539 - macro_f1: 0.7833 - val_loss: 0.5130 - val_accuracy: 0.7521 - val_macro_f1: 0.7803
Epoch 18/23
684/684 - 430s - loss: 0.5057 - accuracy: 0.7550 - macro_f1: 0.7837 - val_loss: 0.5173 - val_accuracy: 0.7487 - val_macro_f1: 0.7745
Epoch 19/23
684/684 - 430s - loss: 0.5051 - accuracy: 0.7557 - macro_f1: 0.7840 - val_loss: 0.5374 - val_accuracy: 0.7320 - val_macro_f1: 0.7463
Epoch 20/23
684/684 - 429s - loss: 0.5036 - accuracy: 0.7565 - macro_f1: 0.7849 - val_loss: 0.5504 - val_accuracy: 0.7196 - val_macro_f1: 0.7304
Epoch 21/23
684/684 - 423s - loss: 0.4989 - accuracy: 0.7592 - macro_f1: 0.7877 - val_loss: 0.5494 - val_accuracy: 0.7203 - val_macro_f1: 0.7274
Epoch 22/23
684/684 - 423s - loss: 0.4933 - accuracy: 0.7629 - macro_f1: 0.7910 - val_loss: 0.4987 - val_accuracy: 0.7594 - val_macro_f1: 0.8033
Epoch 23/23
684/684 - 423s - loss: 0.4874 - accuracy: 0.7667 - macro_f1: 0.7946 - val_loss: 0.5168 - val_accuracy: 0.7440 - val_macro_f1: 0.7695
  File "train_singleTask_CNN_classifier_OCP.py", line 152
    break
    ^
SyntaxError: 'break' outside loop
  File "train_singleTask_CNN_classifier_OCP.py", line 152
    break
    ^
SyntaxError: 'break' outside loop
  File "train_singleTask_CNN_classifier_OCP.py", line 152
    break
    ^
SyntaxError: 'break' outside loop
  File "train_singleTask_CNN_classifier_OCP.py", line 152
    break
    ^
SyntaxError: 'break' outside loop
  File "train_singleTask_CNN_classifier_OCP.py", line 152
    break
    ^
SyntaxError: 'break' outside loop
