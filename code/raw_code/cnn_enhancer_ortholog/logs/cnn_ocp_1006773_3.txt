Running cyclical learning rate for MSN_SN_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.05.
In training mode.
There 174046 positives and 162366 negatives.
There 21926 positives and 20114 negatives.
Epoch 1/23
337/337 - 209s - loss: 0.6908 - accuracy: 0.5152 - macro_f1: 0.3942 - val_loss: 0.6840 - val_accuracy: 0.5432 - val_macro_f1: 0.6328
Epoch 2/23
337/337 - 209s - loss: 0.6826 - accuracy: 0.5508 - macro_f1: 0.4928 - val_loss: 0.6773 - val_accuracy: 0.5730 - val_macro_f1: 0.5296
Epoch 3/23
337/337 - 209s - loss: 0.6769 - accuracy: 0.5698 - macro_f1: 0.5347 - val_loss: 0.6671 - val_accuracy: 0.5912 - val_macro_f1: 0.5864
Epoch 4/23
337/337 - 209s - loss: 0.6691 - accuracy: 0.5853 - macro_f1: 0.5646 - val_loss: 0.6587 - val_accuracy: 0.6042 - val_macro_f1: 0.6550
Epoch 5/23
337/337 - 209s - loss: 0.6616 - accuracy: 0.6020 - macro_f1: 0.5924 - val_loss: 0.6773 - val_accuracy: 0.5785 - val_macro_f1: 0.4360
Epoch 6/23
337/337 - 209s - loss: 0.6545 - accuracy: 0.6130 - macro_f1: 0.6082 - val_loss: 0.6676 - val_accuracy: 0.5909 - val_macro_f1: 0.4747
Epoch 7/23
337/337 - 209s - loss: 0.6495 - accuracy: 0.6208 - macro_f1: 0.6188 - val_loss: 0.6401 - val_accuracy: 0.6350 - val_macro_f1: 0.6472
Epoch 8/23
337/337 - 209s - loss: 0.6437 - accuracy: 0.6281 - macro_f1: 0.6288 - val_loss: 0.6416 - val_accuracy: 0.6288 - val_macro_f1: 0.6927
Epoch 9/23
337/337 - 209s - loss: 0.6400 - accuracy: 0.6343 - macro_f1: 0.6348 - val_loss: 0.6371 - val_accuracy: 0.6412 - val_macro_f1: 0.6399
Epoch 10/23
337/337 - 209s - loss: 0.6376 - accuracy: 0.6367 - macro_f1: 0.6370 - val_loss: 0.6297 - val_accuracy: 0.6465 - val_macro_f1: 0.6752
Epoch 11/23
337/337 - 209s - loss: 0.6351 - accuracy: 0.6399 - macro_f1: 0.6404 - val_loss: 0.6733 - val_accuracy: 0.6051 - val_macro_f1: 0.4847
Epoch 12/23
337/337 - 209s - loss: 0.6310 - accuracy: 0.6435 - macro_f1: 0.6449 - val_loss: 0.6284 - val_accuracy: 0.6494 - val_macro_f1: 0.6760
Epoch 13/23
337/337 - 209s - loss: 0.6273 - accuracy: 0.6482 - macro_f1: 0.6505 - val_loss: 0.6335 - val_accuracy: 0.6421 - val_macro_f1: 0.6988
Epoch 14/23
337/337 - 209s - loss: 0.6252 - accuracy: 0.6501 - macro_f1: 0.6527 - val_loss: 0.6287 - val_accuracy: 0.6448 - val_macro_f1: 0.6278
Epoch 15/23
337/337 - 209s - loss: 0.6219 - accuracy: 0.6535 - macro_f1: 0.6562 - val_loss: 0.6328 - val_accuracy: 0.6433 - val_macro_f1: 0.7034
Epoch 16/23
337/337 - 209s - loss: 0.6194 - accuracy: 0.6566 - macro_f1: 0.6588 - val_loss: 0.6248 - val_accuracy: 0.6538 - val_macro_f1: 0.6659
Epoch 17/23
337/337 - 209s - loss: 0.6170 - accuracy: 0.6589 - macro_f1: 0.6618 - val_loss: 0.6580 - val_accuracy: 0.6210 - val_macro_f1: 0.5302
Epoch 18/23
337/337 - 209s - loss: 0.6132 - accuracy: 0.6634 - macro_f1: 0.6660 - val_loss: 0.6294 - val_accuracy: 0.6452 - val_macro_f1: 0.6179
Epoch 19/23
337/337 - 209s - loss: 0.6098 - accuracy: 0.6665 - macro_f1: 0.6687 - val_loss: 0.6238 - val_accuracy: 0.6521 - val_macro_f1: 0.6737
Epoch 20/23
337/337 - 209s - loss: 0.6044 - accuracy: 0.6718 - macro_f1: 0.6770 - val_loss: 0.6237 - val_accuracy: 0.6561 - val_macro_f1: 0.6839
Epoch 21/23
337/337 - 209s - loss: 0.5993 - accuracy: 0.6763 - macro_f1: 0.6805 - val_loss: 0.6326 - val_accuracy: 0.6479 - val_macro_f1: 0.6260
Epoch 22/23
337/337 - 209s - loss: 0.5930 - accuracy: 0.6821 - macro_f1: 0.6852 - val_loss: 0.6435 - val_accuracy: 0.6328 - val_macro_f1: 0.5950
Epoch 23/23
337/337 - 209s - loss: 0.5866 - accuracy: 0.6878 - macro_f1: 0.6900 - val_loss: 0.6364 - val_accuracy: 0.6471 - val_macro_f1: 0.6591
Running cyclical learning rate for MSN_SN_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.05.
In evaluation mode.
There 21926 positives and 20114 negatives.
43/43 - 5s
Accuracy: 0.6468497778479505. 
f1_score: 0.6471912843059232.
roc_auc: 0.7034621609212782.
prc_auc: 0.7075692997379363.
Running cyclical learning rate for MSN_SN_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.1.
In training mode.
There 174046 positives and 162366 negatives.
There 21926 positives and 20114 negatives.
Epoch 1/23
337/337 - 210s - loss: 0.6905 - accuracy: 0.5189 - macro_f1: 0.4413 - val_loss: 0.6847 - val_accuracy: 0.5387 - val_macro_f1: 0.4717
Epoch 2/23
337/337 - 210s - loss: 0.6821 - accuracy: 0.5524 - macro_f1: 0.5106 - val_loss: 0.6918 - val_accuracy: 0.5192 - val_macro_f1: 0.2109
Epoch 3/23
337/337 - 210s - loss: 0.6740 - accuracy: 0.5750 - macro_f1: 0.5450 - val_loss: 0.6635 - val_accuracy: 0.5972 - val_macro_f1: 0.6831
Epoch 4/23
337/337 - 209s - loss: 0.6662 - accuracy: 0.5933 - macro_f1: 0.5799 - val_loss: 0.6881 - val_accuracy: 0.5455 - val_macro_f1: 0.3044
Epoch 5/23
337/337 - 210s - loss: 0.6590 - accuracy: 0.6061 - macro_f1: 0.6014 - val_loss: 0.6476 - val_accuracy: 0.6270 - val_macro_f1: 0.6799
Epoch 6/23
337/337 - 210s - loss: 0.6541 - accuracy: 0.6158 - macro_f1: 0.6171 - val_loss: 0.6428 - val_accuracy: 0.6328 - val_macro_f1: 0.6498
Epoch 7/23
337/337 - 210s - loss: 0.6492 - accuracy: 0.6217 - macro_f1: 0.6243 - val_loss: 0.6421 - val_accuracy: 0.6337 - val_macro_f1: 0.6238
Epoch 8/23
337/337 - 210s - loss: 0.6453 - accuracy: 0.6273 - macro_f1: 0.6287 - val_loss: 0.6520 - val_accuracy: 0.6221 - val_macro_f1: 0.5619
Epoch 9/23
337/337 - 209s - loss: 0.6428 - accuracy: 0.6310 - macro_f1: 0.6333 - val_loss: 0.6367 - val_accuracy: 0.6400 - val_macro_f1: 0.6277
Epoch 10/23
337/337 - 209s - loss: 0.6380 - accuracy: 0.6363 - macro_f1: 0.6381 - val_loss: 0.6409 - val_accuracy: 0.6275 - val_macro_f1: 0.7012
Epoch 11/23
337/337 - 209s - loss: 0.6357 - accuracy: 0.6391 - macro_f1: 0.6421 - val_loss: 0.6298 - val_accuracy: 0.6479 - val_macro_f1: 0.6650
Epoch 12/23
337/337 - 209s - loss: 0.6326 - accuracy: 0.6422 - macro_f1: 0.6456 - val_loss: 0.6395 - val_accuracy: 0.6341 - val_macro_f1: 0.5947
Epoch 13/23
337/337 - 209s - loss: 0.6318 - accuracy: 0.6427 - macro_f1: 0.6456 - val_loss: 0.6244 - val_accuracy: 0.6523 - val_macro_f1: 0.6677
Epoch 14/23
337/337 - 209s - loss: 0.6297 - accuracy: 0.6460 - macro_f1: 0.6486 - val_loss: 0.6242 - val_accuracy: 0.6533 - val_macro_f1: 0.6779
Epoch 15/23
337/337 - 209s - loss: 0.6249 - accuracy: 0.6508 - macro_f1: 0.6556 - val_loss: 0.6245 - val_accuracy: 0.6541 - val_macro_f1: 0.6660
Epoch 16/23
337/337 - 209s - loss: 0.6228 - accuracy: 0.6532 - macro_f1: 0.6574 - val_loss: 0.6245 - val_accuracy: 0.6564 - val_macro_f1: 0.6556
Epoch 17/23
337/337 - 209s - loss: 0.6197 - accuracy: 0.6564 - macro_f1: 0.6614 - val_loss: 0.6222 - val_accuracy: 0.6565 - val_macro_f1: 0.6598
Epoch 18/23
337/337 - 209s - loss: 0.6189 - accuracy: 0.6571 - macro_f1: 0.6614 - val_loss: 0.6296 - val_accuracy: 0.6460 - val_macro_f1: 0.6135
Epoch 19/23
337/337 - 209s - loss: 0.6134 - accuracy: 0.6630 - macro_f1: 0.6689 - val_loss: 0.6195 - val_accuracy: 0.6590 - val_macro_f1: 0.6899
Epoch 20/23
337/337 - 209s - loss: 0.6143 - accuracy: 0.6614 - macro_f1: 0.6649 - val_loss: 0.6241 - val_accuracy: 0.6558 - val_macro_f1: 0.6697
Epoch 21/23
337/337 - 209s - loss: 0.6181 - accuracy: 0.6571 - macro_f1: 0.6578 - val_loss: 0.6236 - val_accuracy: 0.6535 - val_macro_f1: 0.6943
Epoch 22/23
337/337 - 209s - loss: 0.6054 - accuracy: 0.6710 - macro_f1: 0.6751 - val_loss: 0.6387 - val_accuracy: 0.6395 - val_macro_f1: 0.5943
Epoch 23/23
337/337 - 209s - loss: 0.6015 - accuracy: 0.6735 - macro_f1: 0.6777 - val_loss: 0.6344 - val_accuracy: 0.6427 - val_macro_f1: 0.6081
Running cyclical learning rate for MSN_SN_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.1.
In evaluation mode.
There 21926 positives and 20114 negatives.
43/43 - 5s
Accuracy: 0.6478595199010264. 
f1_score: 0.6382316026536765.
roc_auc: 0.7125858661453849.
prc_auc: 0.7191043842250926.
Running cyclical learning rate for MSN_SN_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.15.
In training mode.
There 174046 positives and 162366 negatives.
There 21926 positives and 20114 negatives.
Epoch 1/23
337/337 - 210s - loss: 0.6897 - accuracy: 0.5221 - macro_f1: 0.4485 - val_loss: 0.6829 - val_accuracy: 0.5505 - val_macro_f1: 0.5932
Epoch 2/23
337/337 - 209s - loss: 0.6822 - accuracy: 0.5517 - macro_f1: 0.5087 - val_loss: 0.6720 - val_accuracy: 0.5843 - val_macro_f1: 0.6019
Epoch 3/23
337/337 - 209s - loss: 0.6752 - accuracy: 0.5724 - macro_f1: 0.5437 - val_loss: 0.6822 - val_accuracy: 0.5577 - val_macro_f1: 0.3737
Epoch 4/23
337/337 - 209s - loss: 0.6664 - accuracy: 0.5921 - macro_f1: 0.5783 - val_loss: 0.6612 - val_accuracy: 0.5980 - val_macro_f1: 0.6904
Epoch 5/23
337/337 - 209s - loss: 0.6611 - accuracy: 0.6037 - macro_f1: 0.5987 - val_loss: 0.6526 - val_accuracy: 0.6242 - val_macro_f1: 0.6377
Epoch 6/23
337/337 - 209s - loss: 0.6566 - accuracy: 0.6115 - macro_f1: 0.6100 - val_loss: 0.6773 - val_accuracy: 0.5609 - val_macro_f1: 0.3707
Epoch 7/23
337/337 - 209s - loss: 0.6539 - accuracy: 0.6153 - macro_f1: 0.6136 - val_loss: 0.6587 - val_accuracy: 0.6066 - val_macro_f1: 0.5356
Epoch 8/23
337/337 - 209s - loss: 0.6485 - accuracy: 0.6232 - macro_f1: 0.6242 - val_loss: 0.6665 - val_accuracy: 0.5964 - val_macro_f1: 0.4828
Epoch 9/23
337/337 - 209s - loss: 0.6449 - accuracy: 0.6276 - macro_f1: 0.6277 - val_loss: 0.6437 - val_accuracy: 0.6325 - val_macro_f1: 0.6014
Epoch 10/23
337/337 - 209s - loss: 0.6435 - accuracy: 0.6290 - macro_f1: 0.6282 - val_loss: 0.6428 - val_accuracy: 0.6328 - val_macro_f1: 0.5973
Epoch 11/23
337/337 - 209s - loss: 0.6402 - accuracy: 0.6338 - macro_f1: 0.6343 - val_loss: 0.6394 - val_accuracy: 0.6403 - val_macro_f1: 0.6893
Epoch 12/23
337/337 - 209s - loss: 0.6386 - accuracy: 0.6350 - macro_f1: 0.6353 - val_loss: 0.6312 - val_accuracy: 0.6444 - val_macro_f1: 0.6896
Epoch 13/23
337/337 - 209s - loss: 0.6352 - accuracy: 0.6397 - macro_f1: 0.6413 - val_loss: 0.6320 - val_accuracy: 0.6460 - val_macro_f1: 0.6391
Epoch 14/23
337/337 - 209s - loss: 0.6340 - accuracy: 0.6403 - macro_f1: 0.6416 - val_loss: 0.6266 - val_accuracy: 0.6476 - val_macro_f1: 0.6620
Epoch 15/23
337/337 - 209s - loss: 0.6305 - accuracy: 0.6448 - macro_f1: 0.6479 - val_loss: 0.6251 - val_accuracy: 0.6513 - val_macro_f1: 0.6598
Epoch 16/23
337/337 - 209s - loss: 0.6291 - accuracy: 0.6458 - macro_f1: 0.6484 - val_loss: 0.6402 - val_accuracy: 0.6312 - val_macro_f1: 0.5666
Epoch 17/23
337/337 - 209s - loss: 0.6289 - accuracy: 0.6460 - macro_f1: 0.6465 - val_loss: 0.6271 - val_accuracy: 0.6507 - val_macro_f1: 0.6961
Epoch 18/23
337/337 - 209s - loss: 0.6259 - accuracy: 0.6498 - macro_f1: 0.6534 - val_loss: 0.6196 - val_accuracy: 0.6568 - val_macro_f1: 0.6926
Epoch 19/23
337/337 - 209s - loss: 0.6239 - accuracy: 0.6521 - macro_f1: 0.6542 - val_loss: 0.6227 - val_accuracy: 0.6558 - val_macro_f1: 0.6403
Epoch 20/23
337/337 - 209s - loss: 0.6206 - accuracy: 0.6559 - macro_f1: 0.6600 - val_loss: 0.6358 - val_accuracy: 0.6410 - val_macro_f1: 0.5879
Epoch 21/23
337/337 - 209s - loss: 0.6177 - accuracy: 0.6590 - macro_f1: 0.6634 - val_loss: 0.6167 - val_accuracy: 0.6625 - val_macro_f1: 0.6945
Epoch 22/23
337/337 - 209s - loss: 0.6170 - accuracy: 0.6584 - macro_f1: 0.6615 - val_loss: 0.6181 - val_accuracy: 0.6601 - val_macro_f1: 0.6507
Epoch 23/23
337/337 - 209s - loss: 0.6118 - accuracy: 0.6642 - macro_f1: 0.6686 - val_loss: 0.6149 - val_accuracy: 0.6648 - val_macro_f1: 0.6926
Running cyclical learning rate for MSN_SN_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.15.
In evaluation mode.
There 21926 positives and 20114 negatives.
43/43 - 5s
Accuracy: 0.6621313856271465. 
f1_score: 0.6633546501581364.
roc_auc: 0.7225561199820152.
prc_auc: 0.7298260595788638.
Running cyclical learning rate for MSN_SN_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.2.
In training mode.
There 174046 positives and 162366 negatives.
There 21926 positives and 20114 negatives.
Epoch 1/23
337/337 - 209s - loss: 0.6931 - accuracy: 0.5045 - macro_f1: 0.3931 - val_loss: 0.6925 - val_accuracy: 0.5142 - val_macro_f1: 0.2264
Epoch 2/23
337/337 - 209s - loss: 0.6852 - accuracy: 0.5413 - macro_f1: 0.4675 - val_loss: 0.6860 - val_accuracy: 0.5404 - val_macro_f1: 0.3643
Epoch 3/23
337/337 - 209s - loss: 0.6799 - accuracy: 0.5606 - macro_f1: 0.5207 - val_loss: 0.6773 - val_accuracy: 0.5730 - val_macro_f1: 0.5305
Epoch 4/23
337/337 - 209s - loss: 0.6726 - accuracy: 0.5789 - macro_f1: 0.5516 - val_loss: 0.6733 - val_accuracy: 0.5872 - val_macro_f1: 0.6663
Epoch 5/23
337/337 - 209s - loss: 0.6668 - accuracy: 0.5921 - macro_f1: 0.5804 - val_loss: 0.6643 - val_accuracy: 0.6035 - val_macro_f1: 0.5477
Epoch 6/23
337/337 - 209s - loss: 0.6610 - accuracy: 0.6032 - macro_f1: 0.5974 - val_loss: 0.6519 - val_accuracy: 0.6187 - val_macro_f1: 0.6078
Epoch 7/23
337/337 - 209s - loss: 0.6581 - accuracy: 0.6083 - macro_f1: 0.6055 - val_loss: 0.6671 - val_accuracy: 0.5871 - val_macro_f1: 0.4664
Epoch 8/23
337/337 - 209s - loss: 0.6530 - accuracy: 0.6172 - macro_f1: 0.6168 - val_loss: 0.6473 - val_accuracy: 0.6226 - val_macro_f1: 0.6685
Epoch 9/23
337/337 - 209s - loss: 0.6501 - accuracy: 0.6209 - macro_f1: 0.6201 - val_loss: 0.6446 - val_accuracy: 0.6365 - val_macro_f1: 0.6518
Epoch 10/23
337/337 - 209s - loss: 0.6464 - accuracy: 0.6263 - macro_f1: 0.6265 - val_loss: 0.6434 - val_accuracy: 0.6277 - val_macro_f1: 0.6919
Epoch 11/23
337/337 - 209s - loss: 0.6431 - accuracy: 0.6305 - macro_f1: 0.6319 - val_loss: 0.6325 - val_accuracy: 0.6415 - val_macro_f1: 0.6692
Epoch 12/23
337/337 - 208s - loss: 0.6403 - accuracy: 0.6336 - macro_f1: 0.6353 - val_loss: 0.6435 - val_accuracy: 0.6265 - val_macro_f1: 0.5756
Epoch 13/23
337/337 - 208s - loss: 0.6392 - accuracy: 0.6348 - macro_f1: 0.6356 - val_loss: 0.6332 - val_accuracy: 0.6440 - val_macro_f1: 0.6602
Epoch 14/23
337/337 - 208s - loss: 0.6380 - accuracy: 0.6361 - macro_f1: 0.6360 - val_loss: 0.6389 - val_accuracy: 0.6361 - val_macro_f1: 0.6962
Epoch 15/23
337/337 - 208s - loss: 0.6347 - accuracy: 0.6399 - macro_f1: 0.6420 - val_loss: 0.6342 - val_accuracy: 0.6394 - val_macro_f1: 0.6929
Epoch 16/23
337/337 - 208s - loss: 0.6329 - accuracy: 0.6418 - macro_f1: 0.6436 - val_loss: 0.6288 - val_accuracy: 0.6523 - val_macro_f1: 0.6704
Epoch 17/23
337/337 - 208s - loss: 0.6304 - accuracy: 0.6458 - macro_f1: 0.6485 - val_loss: 0.6325 - val_accuracy: 0.6420 - val_macro_f1: 0.6977
Epoch 18/23
337/337 - 208s - loss: 0.6296 - accuracy: 0.6451 - macro_f1: 0.6476 - val_loss: 0.6314 - val_accuracy: 0.6471 - val_macro_f1: 0.6170
Epoch 19/23
337/337 - 208s - loss: 0.6292 - accuracy: 0.6458 - macro_f1: 0.6475 - val_loss: 0.6322 - val_accuracy: 0.6436 - val_macro_f1: 0.6147
Epoch 20/23
337/337 - 208s - loss: 0.6283 - accuracy: 0.6477 - macro_f1: 0.6504 - val_loss: 0.6194 - val_accuracy: 0.6587 - val_macro_f1: 0.6732
Epoch 21/23
337/337 - 208s - loss: 0.6267 - accuracy: 0.6490 - macro_f1: 0.6513 - val_loss: 0.6349 - val_accuracy: 0.6407 - val_macro_f1: 0.5940
Epoch 22/23
337/337 - 208s - loss: 0.6218 - accuracy: 0.6544 - macro_f1: 0.6591 - val_loss: 0.6328 - val_accuracy: 0.6420 - val_macro_f1: 0.5931
Epoch 23/23
337/337 - 208s - loss: 0.6203 - accuracy: 0.6558 - macro_f1: 0.6602 - val_loss: 0.6216 - val_accuracy: 0.6546 - val_macro_f1: 0.6311
Running cyclical learning rate for MSN_SN_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.2.
In evaluation mode.
There 21926 positives and 20114 negatives.
43/43 - 5s
Accuracy: 0.6585385903651204. 
f1_score: 0.6523170658183796.
roc_auc: 0.7227468303424289.
prc_auc: 0.7276590446221097.
Running cyclical learning rate for MSN_SN_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.25.
In training mode.
There 174046 positives and 162366 negatives.
There 21926 positives and 20114 negatives.
Epoch 1/23
337/337 - 209s - loss: 0.6927 - accuracy: 0.5086 - macro_f1: 0.4147 - val_loss: 0.6864 - val_accuracy: 0.5340 - val_macro_f1: 0.4589
Epoch 2/23
337/337 - 209s - loss: 0.6857 - accuracy: 0.5391 - macro_f1: 0.4678 - val_loss: 0.6857 - val_accuracy: 0.5408 - val_macro_f1: 0.3839
Epoch 3/23
337/337 - 209s - loss: 0.6776 - accuracy: 0.5661 - macro_f1: 0.5397 - val_loss: 0.6744 - val_accuracy: 0.5765 - val_macro_f1: 0.4628
Epoch 4/23
337/337 - 208s - loss: 0.6774 - accuracy: 0.5686 - macro_f1: 0.5320 - val_loss: 0.6785 - val_accuracy: 0.5674 - val_macro_f1: 0.4347
Epoch 5/23
337/337 - 208s - loss: 0.6675 - accuracy: 0.5895 - macro_f1: 0.5700 - val_loss: 0.6558 - val_accuracy: 0.6176 - val_macro_f1: 0.6643
Epoch 6/23
337/337 - 208s - loss: 0.6650 - accuracy: 0.5961 - macro_f1: 0.5821 - val_loss: 0.6524 - val_accuracy: 0.6201 - val_macro_f1: 0.6627
Epoch 7/23
337/337 - 208s - loss: 0.6581 - accuracy: 0.6088 - macro_f1: 0.6048 - val_loss: 0.6627 - val_accuracy: 0.5952 - val_macro_f1: 0.6943
Epoch 8/23
337/337 - 208s - loss: 0.6554 - accuracy: 0.6130 - macro_f1: 0.6100 - val_loss: 0.6487 - val_accuracy: 0.6298 - val_macro_f1: 0.6393
Epoch 9/23
337/337 - 208s - loss: 0.6514 - accuracy: 0.6190 - macro_f1: 0.6173 - val_loss: 0.6429 - val_accuracy: 0.6315 - val_macro_f1: 0.6805
Epoch 10/23
337/337 - 208s - loss: 0.6476 - accuracy: 0.6253 - macro_f1: 0.6255 - val_loss: 0.6577 - val_accuracy: 0.6090 - val_macro_f1: 0.5143
Epoch 11/23
337/337 - 208s - loss: 0.6454 - accuracy: 0.6276 - macro_f1: 0.6265 - val_loss: 0.6405 - val_accuracy: 0.6350 - val_macro_f1: 0.6809
Epoch 12/23
337/337 - 208s - loss: 0.6429 - accuracy: 0.6304 - macro_f1: 0.6301 - val_loss: 0.6401 - val_accuracy: 0.6394 - val_macro_f1: 0.6282
Epoch 13/23
337/337 - 208s - loss: 0.6398 - accuracy: 0.6346 - macro_f1: 0.6356 - val_loss: 0.6417 - val_accuracy: 0.6338 - val_macro_f1: 0.5886
Epoch 14/23
337/337 - 208s - loss: 0.6374 - accuracy: 0.6368 - macro_f1: 0.6378 - val_loss: 0.6313 - val_accuracy: 0.6469 - val_macro_f1: 0.6700
Epoch 15/23
337/337 - 208s - loss: 0.6373 - accuracy: 0.6368 - macro_f1: 0.6370 - val_loss: 0.6319 - val_accuracy: 0.6484 - val_macro_f1: 0.6528
Epoch 16/23
337/337 - 208s - loss: 0.6378 - accuracy: 0.6354 - macro_f1: 0.6341 - val_loss: 0.6468 - val_accuracy: 0.6275 - val_macro_f1: 0.5590
Epoch 17/23
337/337 - 208s - loss: 0.6349 - accuracy: 0.6397 - macro_f1: 0.6401 - val_loss: 0.6347 - val_accuracy: 0.6440 - val_macro_f1: 0.6308
Epoch 18/23
337/337 - 208s - loss: 0.6318 - accuracy: 0.6441 - macro_f1: 0.6473 - val_loss: 0.6348 - val_accuracy: 0.6427 - val_macro_f1: 0.6070
Epoch 19/23
337/337 - 208s - loss: 0.6297 - accuracy: 0.6454 - macro_f1: 0.6486 - val_loss: 0.6243 - val_accuracy: 0.6544 - val_macro_f1: 0.6612
Epoch 20/23
337/337 - 208s - loss: 0.6300 - accuracy: 0.6452 - macro_f1: 0.6479 - val_loss: 0.6261 - val_accuracy: 0.6553 - val_macro_f1: 0.6464
Epoch 21/23
337/337 - 208s - loss: 0.6259 - accuracy: 0.6498 - macro_f1: 0.6548 - val_loss: 0.6244 - val_accuracy: 0.6559 - val_macro_f1: 0.6561
Epoch 22/23
337/337 - 208s - loss: 0.6259 - accuracy: 0.6496 - macro_f1: 0.6524 - val_loss: 0.6207 - val_accuracy: 0.6582 - val_macro_f1: 0.6741
Epoch 23/23
337/337 - 208s - loss: 0.6315 - accuracy: 0.6439 - macro_f1: 0.6427 - val_loss: 0.6463 - val_accuracy: 0.6208 - val_macro_f1: 0.5297
Running cyclical learning rate for MSN_SN_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.25.
In evaluation mode.
There 21926 positives and 20114 negatives.
43/43 - 5s
Accuracy: 0.6302343766318721. 
f1_score: 0.6030241791924434.
roc_auc: 0.7127861973941818.
prc_auc: 0.7188097753835756.
Running cyclical learning rate for MSN_SN_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.3.
In training mode.
There 174046 positives and 162366 negatives.
There 21926 positives and 20114 negatives.
Epoch 1/23
337/337 - 209s - loss: 0.6903 - accuracy: 0.5199 - macro_f1: 0.4863 - val_loss: 0.6882 - val_accuracy: 0.5300 - val_macro_f1: 0.3457
Epoch 2/23
337/337 - 209s - loss: 0.6823 - accuracy: 0.5528 - macro_f1: 0.5041 - val_loss: 0.7016 - val_accuracy: 0.5005 - val_macro_f1: 0.1113
Epoch 3/23
337/337 - 209s - loss: 0.6754 - accuracy: 0.5717 - macro_f1: 0.5376 - val_loss: 0.6840 - val_accuracy: 0.5412 - val_macro_f1: 0.3042
Epoch 4/23
337/337 - 209s - loss: 0.6678 - accuracy: 0.5902 - macro_f1: 0.5732 - val_loss: 0.6812 - val_accuracy: 0.5586 - val_macro_f1: 0.3688
Epoch 5/23
337/337 - 209s - loss: 0.6628 - accuracy: 0.6001 - macro_f1: 0.5921 - val_loss: 0.6624 - val_accuracy: 0.6004 - val_macro_f1: 0.5496
Epoch 6/23
337/337 - 208s - loss: 0.6587 - accuracy: 0.6083 - macro_f1: 0.6048 - val_loss: 0.6721 - val_accuracy: 0.5758 - val_macro_f1: 0.4289
Epoch 7/23
337/337 - 208s - loss: 0.6548 - accuracy: 0.6143 - macro_f1: 0.6154 - val_loss: 0.6777 - val_accuracy: 0.5725 - val_macro_f1: 0.4076
Epoch 8/23
337/337 - 208s - loss: 0.6524 - accuracy: 0.6169 - macro_f1: 0.6164 - val_loss: 0.6544 - val_accuracy: 0.6192 - val_macro_f1: 0.6907
Epoch 9/23
337/337 - 208s - loss: 0.6508 - accuracy: 0.6212 - macro_f1: 0.6197 - val_loss: 0.6617 - val_accuracy: 0.5978 - val_macro_f1: 0.4938
Epoch 10/23
337/337 - 208s - loss: 0.6481 - accuracy: 0.6242 - macro_f1: 0.6232 - val_loss: 0.6479 - val_accuracy: 0.6233 - val_macro_f1: 0.5775
Epoch 11/23
337/337 - 208s - loss: 0.6462 - accuracy: 0.6261 - macro_f1: 0.6247 - val_loss: 0.6366 - val_accuracy: 0.6390 - val_macro_f1: 0.6693
Epoch 12/23
337/337 - 208s - loss: 0.6424 - accuracy: 0.6317 - macro_f1: 0.6324 - val_loss: 0.6336 - val_accuracy: 0.6440 - val_macro_f1: 0.6592
Epoch 13/23
337/337 - 208s - loss: 0.6419 - accuracy: 0.6314 - macro_f1: 0.6292 - val_loss: 0.6346 - val_accuracy: 0.6444 - val_macro_f1: 0.6739
Epoch 14/23
337/337 - 208s - loss: 0.6380 - accuracy: 0.6363 - macro_f1: 0.6387 - val_loss: 0.6316 - val_accuracy: 0.6475 - val_macro_f1: 0.6674
Epoch 15/23
337/337 - 208s - loss: 0.6373 - accuracy: 0.6374 - macro_f1: 0.6376 - val_loss: 0.6365 - val_accuracy: 0.6360 - val_macro_f1: 0.6904
Epoch 16/23
337/337 - 208s - loss: 0.6355 - accuracy: 0.6387 - macro_f1: 0.6410 - val_loss: 0.6331 - val_accuracy: 0.6458 - val_macro_f1: 0.6317
Epoch 17/23
337/337 - 208s - loss: 0.6339 - accuracy: 0.6408 - macro_f1: 0.6431 - val_loss: 0.6343 - val_accuracy: 0.6404 - val_macro_f1: 0.6103
Epoch 18/23
337/337 - 208s - loss: 0.6340 - accuracy: 0.6404 - macro_f1: 0.6420 - val_loss: 0.6297 - val_accuracy: 0.6432 - val_macro_f1: 0.6991
Epoch 19/23
337/337 - 208s - loss: 0.6357 - accuracy: 0.6391 - macro_f1: 0.6383 - val_loss: 0.6337 - val_accuracy: 0.6418 - val_macro_f1: 0.6188
Epoch 20/23
337/337 - 208s - loss: 0.6354 - accuracy: 0.6396 - macro_f1: 0.6387 - val_loss: 0.6289 - val_accuracy: 0.6511 - val_macro_f1: 0.6452
Epoch 21/23
337/337 - 208s - loss: 0.6292 - accuracy: 0.6468 - macro_f1: 0.6513 - val_loss: 0.6280 - val_accuracy: 0.6485 - val_macro_f1: 0.6262
Epoch 22/23
337/337 - 208s - loss: 0.6293 - accuracy: 0.6453 - macro_f1: 0.6468 - val_loss: 0.6275 - val_accuracy: 0.6546 - val_macro_f1: 0.6682
Epoch 23/23
337/337 - 208s - loss: 0.6254 - accuracy: 0.6509 - macro_f1: 0.6560 - val_loss: 0.6210 - val_accuracy: 0.6603 - val_macro_f1: 0.6720
Running cyclical learning rate for MSN_SN_hgRmMm_enhVsNonEnhOrth.
One-cycle policy training for 23 epochs.
Learning rates range: 0.01 - 0.1.
Momentum rates range: 0.85 - 0.99.
Dropout: 0.3.
In evaluation mode.
There 21926 positives and 20114 negatives.
43/43 - 5s
Accuracy: 0.6600092960955355. 
f1_score: 0.6603756824242434.
roc_auc: 0.7168159573982074.
prc_auc: 0.7239208964012983.
